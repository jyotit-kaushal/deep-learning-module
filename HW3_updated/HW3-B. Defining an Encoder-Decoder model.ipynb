{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aadb0533-3bd7-4fc5-9009-b6d7488fc801",
   "metadata": {},
   "source": [
    "# HW3-B. Defining an Encoder-Decoder model\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (06/03/2024)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3\n",
    "- Matplotlib\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Torch\n",
    "- Torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e7938d-4c76-437f-9c5f-c7dc707a5078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/sx3slfp54xv_8syv33c8kzz80000gn/T/ipykernel_57523/3089721377.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Helper functions (additional file)\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d28b7-d157-413d-9870-9c4bc8f656cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>An important note: While usually not advised, you might want to run the code for this homework using CPU only. <br>\n",
    "It remains possible, however, to use GPU, but we would advise against it, until we have been able to clarify the reason for bugs (most likely some CUDA reason). </b> \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd236cf0-f1be-48dd-afad-84ddd8196eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b221d4-4c60-447f-99d9-9eb35d7ee805",
   "metadata": {},
   "source": [
    "## 0. Dataset and Dataloaders from earlier\n",
    "\n",
    "We start by loading our dataset from the Excel file, and reuse our Dataset and Dataloader objects from HW3-A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989386df-db14-4c13-80f0-8ec7f2d24161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from file\n",
    "excel_file_path = 'dataset.xlsx'\n",
    "times, values = load_dataset(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5058d945-242b-4748-8977-3f4335fdce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, times, values, n_inputs, n_outputs):\n",
    "        self.times = times\n",
    "        self.values = values\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.define_samples()\n",
    "\n",
    "    def define_samples(self):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.mid = []\n",
    "        # Define all inputs\n",
    "        for i in range(len(times) - self.n_inputs - self.n_outputs + 1):\n",
    "            # Last input not included (only 19 values)\n",
    "            next_input = self.values[i:(i + self.n_inputs - 1)]\n",
    "            next_output = self.values[(i + self.n_inputs):(i + self.n_inputs + self.n_outputs)]\n",
    "            # Mid is the turning point, i.e. the value of the 20-th sample in the series of inputs\n",
    "            # It will not be read by the encoder and will serve as the first input to the decoder\n",
    "            next_mid = [self.values[i + self.n_inputs - 1]]\n",
    "            self.inputs.append(next_input)\n",
    "            self.outputs.append(next_output)\n",
    "            self.mid.append(next_mid)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Select samples corresponding to the different inputs\n",
    "        # and outputs we have created with the define_samples() function,\n",
    "        # and convert them to PyTorch tensors\n",
    "        x = torch.tensor(self.inputs[idx], dtype = torch.float32)\n",
    "        y = torch.tensor(self.outputs[idx], dtype = torch.float32)\n",
    "        m = torch.tensor(self.mid[idx], dtype = torch.float32)\n",
    "        return x, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa1b5a1-c46e-47f9-9b71-5730224c6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our PyTorch Dataset object from the class above\n",
    "n_inputs = 20\n",
    "n_outputs = 5\n",
    "pt_dataset = CustomDataset(times, values, n_inputs, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b8fef6-6c48-4c91-b965-6fbcfeb87a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader object\n",
    "batch_size = 256\n",
    "pt_dataloader = DataLoader(pt_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa6d1f-effd-472e-a140-14c71c441979",
   "metadata": {},
   "source": [
    "## 1. Step 1: Designing an Encoder model\n",
    "\n",
    "We propose to approach this task, by using and Encoder-Decoder model of some sort. Both the Encoder and Decoder parts of the model will consist of a simple LSTM.\n",
    "\n",
    "**Question 6:** What is a Seq2Seq model, and how does it relate to Encoder-Decoder models?\n",
    "\n",
    "A Seq2Seq model as the name suggests essentially helps map one sequence (i.e. the feature sequence) to another different sequence which is your target sequence. An example of sequence-to-sequence model could be a translation model for example. It relates to the encoder-decoder models in the sense that the seq2seq model itself has 2 main components as part of it's architecture. \n",
    "\n",
    "An encoder first processes the input sequence using RNNs and basically encodes the input sequence into a context vector with fixed dimensions and we can see this as the semantics of the input sequence. \n",
    "\n",
    "The decoder then takes the context vector as generated by the encoder above and again using RNNs produces the output/target sequence one by one to gives us the output sequence. \n",
    "\n",
    "Therefore, we can pretty much see the Seq2Seq model as an instance of the Encoder-Decoder models where the overall model is divided into 2 components of and Encoder and a Decoder. \n",
    "\n",
    "We want to implement the LSTM architecture drawn below. It objective is to receives entires $ x(t), x(t+1), ..., x(t+18) $, 19 input points, and learn the dynamics of the data, in the hopes that we will later be able to use this information for future predictions.\n",
    "\n",
    "<img title=\"Our Encoder Architecture\" alt=\"Our Encoder Architecture\" src=\"./images/20240318_183921.jpg\">\n",
    "\n",
    "Given the LSTM Architecture above, answer the questions below.\n",
    "\n",
    "**Question 7:** This encoder seems to receive all inputs present in the first tensor coming from the Dataloader object, which includes n_inputs - 1 elements (here 20-1 = 19 inputs). This LSTM could then produce 19 outputs, but for some reason, they are not shown on this image. What is the reason for this omission? Why is our diagram suggesting that the final memory vector is the only important information that will come out of this encoder model?\n",
    "\n",
    "The main reason to do so is basically for the clarity of the diagram we are presenting and to essentially focus on only showing the key components and outputs of the encoder which in this case would be the final memory vector as pointed out. It's true yes that the encoder produces an output which is \"hidden\" upon recieving each of the elements in the sesquence but the focus is still on the main memory vector that is going to be passed through to the decoder. Therefore, this functionality of the encoder is abstracted and the diagram is as it is shown.\n",
    "\n",
    "We want our Encoder model to be represented by the EncoderRNN object, whose class prototype is shown below.\n",
    "\n",
    "**Question 8:** There are a few Nones to be replaced in the code below. Please show your code in your report after you have figured out the correct EncoderRNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f68a92b-8441-454c-8e26-dfa7d4349f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         batch_size = inputs.size(0)  # Get batch size from the first dimension\n",
    "#         hidden = torch.randn(1, batch_size, self.hidden_size)\n",
    "#         cell = torch.randn(1, batch_size, self.hidden_size)\n",
    "        \n",
    "#         # Pass inputs through the LSTM layer\n",
    "#         output, (hidden, cell) = self.lstm(inputs, (hidden, cell))\n",
    "        \n",
    "#         # Return the output and hidden states\n",
    "#         return output, hidden\n",
    "\n",
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         # Get batch size from the first dimension of the input\n",
    "#         batch_size = inputs.size(0)\n",
    "\n",
    "#         # Initialize hidden and cell states with zeros\n",
    "#         hidden = torch.zeros(1, batch_size, self.hidden_size)\n",
    "#         cell = torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "#         # Pass inputs through the LSTM layer\n",
    "#         output, (hidden, cell) = self.lstm(inputs, (hidden, cell))\n",
    "\n",
    "#         # Return the output and the last hidden state\n",
    "#         return output\n",
    "\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get batch size from the first dimension of the input (might be 1 for unbatched case)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        # Initialize hidden and cell states with zeros (ignoring batch dimension)\n",
    "        hidden = torch.zeros(self.hidden_size)\n",
    "        cell = torch.zeros(self.hidden_size)\n",
    "\n",
    "        # Pass inputs through the LSTM layer\n",
    "        output, (hidden, cell) = self.lstm(inputs, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        # Return the output and the last hidden state\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c6a974a-f548-4109-bbba-110e8e89b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (lstm): LSTM(20, 25)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Defining our EncoderRNN model\n",
    "hidden_size = 25\n",
    "encoder_model = EncoderRNN(n_inputs, hidden_size).to(device)\n",
    "print(encoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf65f1c-647d-431c-8ae8-6c885a6861d0",
   "metadata": {},
   "source": [
    "**Question 9:** Consider the cell below. What is contained in *vec1\\[0\\]* and *vec2\\[0\\]*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a177835-115b-4a48-85b9-fe954779dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs_reworked \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs_reworked\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_reworked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m vec1, vec2 \u001b[38;5;241m=\u001b[39m encoder_out\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(vec1[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/datasci/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datasci/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 55\u001b[0m, in \u001b[0;36mEncoderRNN.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m cell \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Pass inputs through the LSTM layer\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Return the output and the last hidden state\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[0;32m~/miniconda3/envs/datasci/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datasci/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datasci/lib/python3.12/site-packages/torch/nn/modules/rnn.py:874\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    871\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/datasci/lib/python3.12/site-packages/torch/nn/modules/rnn.py:789\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    785\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    786\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    787\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    788\u001b[0m                        ):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    791\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    793\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/datasci/lib/python3.12/site-packages/torch/nn/modules/rnn.py:239\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 20, got 19"
     ]
    }
   ],
   "source": [
    "# Testing our EncoderRNN model\n",
    "inputs, _, _ = next(iter(pt_dataloader))\n",
    "inputs_reworked = inputs[0, :].reshape(1, -1)\n",
    "print(inputs_reworked.shape)\n",
    "encoder_out = encoder_model(inputs_reworked)\n",
    "vec1, vec2 = encoder_out\n",
    "print(vec1[0])\n",
    "print(vec2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636717f-8667-4cdd-ad32-38187b2abc68",
   "metadata": {},
   "source": [
    "## 2. Step 2: Designing a Decoder model\n",
    "\n",
    "Our next step is to produce a decoder model. It will receive a certain memory vector as its memory starting point. It will also receive five inputs denoted *val1*, *val2*, *val3*, *val4*, and *val5*. It will then attempt to produce five outputs denoted *y1*, *y2*, *y3*, *y4*, and *y5*.\n",
    "\n",
    "Consider the architecture drawn below and answer the following questions.\n",
    "\n",
    "As you can see, it will receive certain input values *valk* and will attempt to predict a value *yk*, with k in $ \\{1, 2, 3, 4, 5\\} $.\n",
    "\n",
    "<img title=\"Our Decoder Architecture\" alt=\"Our Decoder Architecture\" src=\"./images/20240318_184112.jpg\">\n",
    "\n",
    "**Question 10:** Assuming that the encoder has seen the inputs $ x(t), x(t+1), ... x(t+18) $, what should we use as a memory vector to play the role of the memory starting point for the decoder?\n",
    "\n",
    "**Question 11:** We will use a Decoder that is NOT auto-regressive. What does that mean for the input and output values of our Decoder LSTM-based model?\n",
    "\n",
    "**Question 12:** Assuming that the encoder has seen the inputs corresponding to the sample with index $ t $, i.e. $ x(t), x(t+1), ... x(t+18) $, which values should we use in place fo *val1*, *val2*, *val3*, *val4*, *val5*? Remember Q11, we are not planning to use an auto-regressive decoder here. Could you then explain why we only used 19 values as inputs in the encoder part then?\n",
    "\n",
    "**Question 13:** Assuming that the encoder has seen the inputs corresponding to the sample with index $ t $, i.e. $ x(t), x(t+1), ... x(t+18) $, what are the target values should we are trying to match with our predictions in place fo *y1*, *y2*, *y3*, *y4*, *y5*?\n",
    "\n",
    "**Question 14:** What is then the purpose and the expected use for the Linear layer in self.linear? Why is there a for loop in the forward method?\n",
    "\n",
    "**Question 15:** Having figured out the questions in Q10-14, can you figure what to use in place of the Nones in the code for the DecoderRNN below? Show your final code in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f97ed-53cb-44ab-b7d9-ddc0f0b07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = None\n",
    "        self.linear = nn.Linear(None)\n",
    "\n",
    "    def forward(self, outputs, mid, encoder_hidden_states):\n",
    "        hidden_states = encoder_hidden_states\n",
    "        final_pred = torch.zeros(outputs.shape).to(outputs.device)\n",
    "        val = None\n",
    "        for i in range(outputs.shape[1]):\n",
    "            pred, hidden_states = None\n",
    "            pred = self.linear(pred)\n",
    "            final_pred[:, i] = pred.squeeze()\n",
    "            val = None\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4fb7a7-d01c-4515-b878-fb12733c4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our DecoderRNN model\n",
    "decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = n_outputs)\n",
    "print(decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0454373-5fa0-457d-bc07-b053529a63fc",
   "metadata": {},
   "source": [
    "**Question 16:** Consider the cell below. What should the final size of the *decoder_out* tensor be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccb5dc-b17a-45dc-84b2-3ffbe1c0536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our DecoderRNN model\n",
    "inputs, outputs, mid = next(iter(pt_dataloader))\n",
    "encoder_out = encoder_model(inputs)\n",
    "decoder_out = decoder_model(outputs, mid, encoder_out)\n",
    "print(decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b5597-f513-4bc8-ac44-2126b2b8eb7b",
   "metadata": {},
   "source": [
    "## Step 3: Assembling everything into a Seq2Seq model.\n",
    "\n",
    "Our final objective is to assemble both our encoder model and decoder model into a Seq2Seq model, following the architecture drawn below.\n",
    "\n",
    "<img title=\"Our Seq2Seq Architecture\" alt=\"Our Seq2Seq Architecture\" src=\"./images/20240318_184350.jpg\">\n",
    "\n",
    "**Question 17:** Why have we prefered to use a Decoder-Encoder architecture, instead of a single LSTM that would receive 24 inputs, produce 24 outputs, and would only compare the final 5 predicted values to the ground truth in our dataset?\n",
    "\n",
    "**Question 18:** Having figured out the models in EncoderRNN and DecoderRNN, can you now figure out the missing code in the cell below? Show it in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41220699-b426-4abe-aac3-84f4849d96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.encoder_model = EncoderRNN(self.input_size, self.hidden_size)\n",
    "        self.decoder_model = DecoderRNN(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, inputs, outputs, mid):\n",
    "        encoder_hidden_states = self.encoder_model(inputs)\n",
    "        pred_final = self.decoder_model(outputs, mid, encoder_hidden_states)\n",
    "        return pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81097bd5-99bd-4d68-b7f5-a2e321171ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our Seq2Seq model\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs)\n",
    "print(seq2seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abde92-6963-4ec6-9890-642675d14a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our Seq2Seq model\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs)\n",
    "inputs, outputs, mid = next(iter(pt_dataloader))\n",
    "print(inputs.shape, outputs.shape, mid.shape)\n",
    "seq2seq_out = seq2seq_model(inputs, outputs, mid)\n",
    "print(seq2seq_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c9523-6fd0-44f6-8605-fd954f35153d",
   "metadata": {},
   "source": [
    "## Step 4: Finally, training and evaluating our Seq2Seq model\n",
    "\n",
    "**Question 19:** Given your understanding of the task, which (very simple) loss function should we use in our trainer function? Show your updated code in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9546c6c-8a67-40de-aea4-156936326ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, num_epochs, learning_rate):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, outputs, mid in dataloader:\n",
    "            # Clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            pred = model(inputs, outputs, mid)\n",
    "            # Calculate loss\n",
    "            loss = criterion(pred, outputs)\n",
    "            total_loss += loss.item()\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Print total loss every few epochs\n",
    "        if epoch % 25 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {total_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efb97f-51b2-4bdf-8927-717d17be468f",
   "metadata": {},
   "source": [
    "Having figure out the correct models and trainer function, you may not use the celml below. It will train the model from scratch, on 50 iterations and will show you the amount of time take to train this model. This is just information to let you know how long the full training loop (in the next cell), might take on your machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83d036-543f-4970-892a-ea0e7d6ef807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "%timeit -r 1 -n 1 train(dataloader = pt_dataloader, model = seq2seq_model, num_epochs = 26, learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b1d43-94f9-48a2-b6d8-16810a7d5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was used to save a starting point for the next cell, do not run.\n",
    "#torch.save(seq2seq_model.state_dict(), 'seq2seq_model_start.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4454380-e706-47e1-8b19-6b51342b338a",
   "metadata": {},
   "source": [
    "**Question 20:** It seems the loss values we are seeing when using the model with randomly initialized parameters is very high. While it seems to decrease, it seems lots of iterations will be needed. The next cell suggests to run the training loop, but initialize the weights of the model using values in the *seq2seq_model_start.pth* file, presumably coming from another roughly similar model, trained on a different but similar task. This is done in an attempt to help the model train better and faster. Under which name is this concept known in Deep Learning?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note: The next cell will take a (somewhat) long time to run. On my machine CPU, it takes ~4minutes.<br>\n",
    "You can simply guess how long it will take to run on your machine, by using the execution time of the previous cell (using 50 iterations) and multiplying that by 20. As mentioned at the beginning of this Notebook, we have observed issues in running the code, on some CUDA machines. It is unclear at the moment, so try GPU computing at your own risk... </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c034a-03ba-46a1-b369-121f9756fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "# Start from given parameters to make training easier\n",
    "seq2seq_model.load_state_dict(torch.load('seq2seq_model_start.pth'))\n",
    "%timeit -r 1 -n 1 train(dataloader = pt_dataloader, model = seq2seq_model, num_epochs = 1051, learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa2893-acc6-4985-8556-14a5acfe403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not uncomment and execute, this was used to prepare the model that you will be using in the final question!\n",
    "#torch.save(seq2seq_model.state_dict(), 'seq2seq_model_end.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb861c49-f721-410d-b71a-56f0a8dc18ec",
   "metadata": {},
   "source": [
    "**Question 21:** The code below shows the predictions produced by your Seq2Seq model after training and can be used to confirm that you have trained the right model! Show some screenshots in your report, and discuss the final performance you have obtained for your model. For your information, I typically obtain an MSE of ~0.05 after 1000 iterations of training. Additional performance can probably be obtained via hyperparameters tuning (changing the size of memory vector, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facc4e8-e27b-4844-881f-20b48daff13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on our Seq2Seq model\n",
    "# (Seeding for reproducibility)\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "seq2seq_model.load_state_dict(torch.load('seq2seq_model_end.pth'))\n",
    "seed_value = 187\n",
    "test_model(seq2seq_model, pt_dataloader, seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e87e31-7dd3-4af4-9c5e-044fbde05567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "# (Seeding for reproducibility)\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "seq2seq_model.load_state_dict(torch.load('seq2seq_model_end.pth'))\n",
    "# Visualize\n",
    "visualize_some_predictions(seq2seq_model, pt_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442801e-e21e-4721-8e13-b789c14ddca5",
   "metadata": {},
   "source": [
    "## This concludes HW3.\n",
    "\n",
    "Do not give up, it is a feasible task! If your model does not work, most likely, you are making a mistake in the Encoder model or - most likely - the Decoder model. Take your time to think about the task at hand and the model we should use for that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e0296-cd6f-40f5-a4d1-a3e88ec77b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
