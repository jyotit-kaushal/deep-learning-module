{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aadb0533-3bd7-4fc5-9009-b6d7488fc801",
   "metadata": {},
   "source": [
    "# HW3-B. Defining an Encoder-Decoder model\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (06/03/2024)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3\n",
    "- Matplotlib\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Torch\n",
    "- Torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7938d-4c76-437f-9c5f-c7dc707a5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Helper functions (additional file)\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d28b7-d157-413d-9870-9c4bc8f656cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>An important note: While usually not advised, you might want to run the code for this homework using CPU only. <br>\n",
    "It remains possible, however, to use GPU, but we would advise against it, until we have been able to clarify the reason for bugs (most likely some CUDA reason). </b> \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd236cf0-f1be-48dd-afad-84ddd8196eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b221d4-4c60-447f-99d9-9eb35d7ee805",
   "metadata": {},
   "source": [
    "## 0. Dataset and Dataloaders from earlier\n",
    "\n",
    "We start by loading our dataset from the Excel file, and reuse our Dataset and Dataloader objects from HW3-A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989386df-db14-4c13-80f0-8ec7f2d24161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from file\n",
    "excel_file_path = 'dataset.xlsx'\n",
    "times, values = load_dataset(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058d945-242b-4748-8977-3f4335fdce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, times, values, n_inputs, n_outputs):\n",
    "        self.times = times\n",
    "        self.values = values\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.define_samples()\n",
    "\n",
    "    def define_samples(self):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.mid = []\n",
    "        # Define all inputs\n",
    "        for i in range(len(times) - self.n_inputs - self.n_outputs + 1):\n",
    "            # Last input not included (only 19 values)\n",
    "            next_input = self.values[i:(i + self.n_inputs - 1)]\n",
    "            next_output = self.values[(i + self.n_inputs):(i + self.n_inputs + self.n_outputs)]\n",
    "            # Mid is the turning point, i.e. the value of the 20-th sample in the series of inputs\n",
    "            # It will not be read by the encoder and will serve as the first input to the decoder\n",
    "            next_mid = [self.values[i + self.n_inputs - 1]]\n",
    "            self.inputs.append(next_input)\n",
    "            self.outputs.append(next_output)\n",
    "            self.mid.append(next_mid)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Select samples corresponding to the different inputs\n",
    "        # and outputs we have created with the define_samples() function,\n",
    "        # and convert them to PyTorch tensors\n",
    "        x = torch.tensor(self.inputs[idx], dtype = torch.float32)\n",
    "        y = torch.tensor(self.outputs[idx], dtype = torch.float32)\n",
    "        m = torch.tensor(self.mid[idx], dtype = torch.float32)\n",
    "        return x, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1b5a1-c46e-47f9-9b71-5730224c6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our PyTorch Dataset object from the class above\n",
    "n_inputs = 20\n",
    "n_outputs = 5\n",
    "pt_dataset = CustomDataset(times, values, n_inputs, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8fef6-6c48-4c91-b965-6fbcfeb87a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader object\n",
    "batch_size = 256\n",
    "pt_dataloader = DataLoader(pt_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa6d1f-effd-472e-a140-14c71c441979",
   "metadata": {},
   "source": [
    "## 1. Step 1: Designing an Encoder model\n",
    "\n",
    "We propose to approach this task, by using and Encoder-Decoder model of some sort. Both the Encoder and Decoder parts of the model will consist of a simple LSTM.\n",
    "\n",
    "**Question 6:** What is a Seq2Seq model, and how does it relate to Encoder-Decoder models?\n",
    "\n",
    "We want to implement the LSTM architecture drawn below. It objective is to receives entires $ x(t), x(t+1), ..., x(t+18) $, 19 input points, and learn the dynamics of the data, in the hopes that we will later be able to use this information for future predictions.\n",
    "\n",
    "<img title=\"Our Encoder Architecture\" alt=\"Our Encoder Architecture\" src=\"./images/20240318_183921.jpg\">\n",
    "\n",
    "Given the LSTM Architecture above, answer the questions below.\n",
    "\n",
    "**Question 7:** This encoder seems to receive all inputs present in the first tensor coming from the Dataloader object, which includes n_inputs - 1 elements (here 20-1 = 19 inputs). This LSTM could then produce 19 outputs, but for some reason, they are not shown on this image. What is the reason for this omission? Why is our diagram suggesting that the final memory vector is the only important information that will come out of this encoder model?\n",
    "\n",
    "We want our Encoder model to be represented by the EncoderRNN object, whose class prototype is shown below.\n",
    "\n",
    "**Question 8:** There are a few Nones to be replaced in the code below. Please show your code in your report after you have figured out the correct EncoderRNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68a92b-8441-454c-8e26-dfa7d4349f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output, hidden = None\n",
    "        # Return the only useful information that can be used by the decoder\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a974a-f548-4109-bbba-110e8e89b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our EncoderRNN model\n",
    "hidden_size = 25\n",
    "encoder_model = EncoderRNN(n_inputs, hidden_size).to(device)\n",
    "print(encoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf65f1c-647d-431c-8ae8-6c885a6861d0",
   "metadata": {},
   "source": [
    "**Question 9:** Consider the cell below. What is contained in *vec1\\[0\\]* and *vec2\\[0\\]*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a177835-115b-4a48-85b9-fe954779dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our EncoderRNN model\n",
    "inputs, _, _ = next(iter(pt_dataloader))\n",
    "inputs_reworked = inputs[0, :].reshape(1, -1)\n",
    "print(inputs_reworked.shape)\n",
    "encoder_out = encoder_model(inputs_reworked)\n",
    "vec1, vec2 = encoder_out\n",
    "print(vec1[0])\n",
    "print(vec2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636717f-8667-4cdd-ad32-38187b2abc68",
   "metadata": {},
   "source": [
    "## 2. Step 2: Designing a Decoder model\n",
    "\n",
    "Our next step is to produce a decoder model. It will receive a certain memory vector as its memory starting point. It will also receive five inputs denoted *val1*, *val2*, *val3*, *val4*, and *val5*. It will then attempt to produce five outputs denoted *y1*, *y2*, *y3*, *y4*, and *y5*.\n",
    "\n",
    "Consider the architecture drawn below and answer the following questions.\n",
    "\n",
    "As you can see, it will receive certain input values *valk* and will attempt to predict a value *yk*, with k in $ \\{1, 2, 3, 4, 5\\} $.\n",
    "\n",
    "<img title=\"Our Decoder Architecture\" alt=\"Our Decoder Architecture\" src=\"./images/20240318_184112.jpg\">\n",
    "\n",
    "**Question 10:** Assuming that the encoder has seen the inputs $ x(t), x(t+1), ... x(t+18) $, what should we use as a memory vector to play the role of the memory starting point for the decoder?\n",
    "\n",
    "**Question 11:** We will use a Decoder that is NOT auto-regressive. What does that mean for the input and output values of our Decoder LSTM-based model?\n",
    "\n",
    "**Question 12:** Assuming that the encoder has seen the inputs corresponding to the sample with index $ t $, i.e. $ x(t), x(t+1), ... x(t+18) $, which values should we use in place fo *val1*, *val2*, *val3*, *val4*, *val5*? Remember Q11, we are not planning to use an auto-regressive decoder here. Could you then explain why we only used 19 values as inputs in the encoder part then?\n",
    "\n",
    "**Question 13:** Assuming that the encoder has seen the inputs corresponding to the sample with index $ t $, i.e. $ x(t), x(t+1), ... x(t+18) $, what are the target values should we are trying to match with our predictions in place fo *y1*, *y2*, *y3*, *y4*, *y5*?\n",
    "\n",
    "**Question 14:** What is then the purpose and the expected use for the Linear layer in self.linear? Why is there a for loop in the forward method?\n",
    "\n",
    "**Question 15:** Having figured out the questions in Q10-14, can you figure what to use in place of the Nones in the code for the DecoderRNN below? Show your final code in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f97ed-53cb-44ab-b7d9-ddc0f0b07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = None\n",
    "        self.linear = nn.Linear(None)\n",
    "\n",
    "    def forward(self, outputs, mid, encoder_hidden_states):\n",
    "        hidden_states = encoder_hidden_states\n",
    "        final_pred = torch.zeros(outputs.shape).to(outputs.device)\n",
    "        val = None\n",
    "        for i in range(outputs.shape[1]):\n",
    "            pred, hidden_states = None\n",
    "            pred = self.linear(pred)\n",
    "            final_pred[:, i] = pred.squeeze()\n",
    "            val = None\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4fb7a7-d01c-4515-b878-fb12733c4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our DecoderRNN model\n",
    "decoder_model = DecoderRNN(hidden_size = hidden_size, output_size = n_outputs)\n",
    "print(decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0454373-5fa0-457d-bc07-b053529a63fc",
   "metadata": {},
   "source": [
    "**Question 16:** Consider the cell below. What should the final size of the *decoder_out* tensor be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccb5dc-b17a-45dc-84b2-3ffbe1c0536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our DecoderRNN model\n",
    "inputs, outputs, mid = next(iter(pt_dataloader))\n",
    "encoder_out = encoder_model(inputs)\n",
    "decoder_out = decoder_model(outputs, mid, encoder_out)\n",
    "print(decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b5597-f513-4bc8-ac44-2126b2b8eb7b",
   "metadata": {},
   "source": [
    "## Step 3: Assembling everything into a Seq2Seq model.\n",
    "\n",
    "Our final objective is to assemble both our encoder model and decoder model into a Seq2Seq model, following the architecture drawn below.\n",
    "\n",
    "<img title=\"Our Seq2Seq Architecture\" alt=\"Our Seq2Seq Architecture\" src=\"./images/20240318_184350.jpg\">\n",
    "\n",
    "**Question 17:** Why have we prefered to use a Decoder-Encoder architecture, instead of a single LSTM that would receive 24 inputs, produce 24 outputs, and would only compare the final 5 predicted values to the ground truth in our dataset?\n",
    "\n",
    "**Question 18:** Having figured out the models in EncoderRNN and DecoderRNN, can you now figure out the missing code in the cell below? Show it in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41220699-b426-4abe-aac3-84f4849d96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.encoder_model = EncoderRNN(self.input_size, self.hidden_size)\n",
    "        self.decoder_model = DecoderRNN(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, inputs, outputs, mid):\n",
    "        encoder_hidden_states = self.encoder_model(inputs)\n",
    "        pred_final = self.decoder_model(outputs, mid, encoder_hidden_states)\n",
    "        return pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81097bd5-99bd-4d68-b7f5-a2e321171ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our Seq2Seq model\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs)\n",
    "print(seq2seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abde92-6963-4ec6-9890-642675d14a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our Seq2Seq model\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs)\n",
    "inputs, outputs, mid = next(iter(pt_dataloader))\n",
    "print(inputs.shape, outputs.shape, mid.shape)\n",
    "seq2seq_out = seq2seq_model(inputs, outputs, mid)\n",
    "print(seq2seq_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c9523-6fd0-44f6-8605-fd954f35153d",
   "metadata": {},
   "source": [
    "## Step 4: Finally, training and evaluating our Seq2Seq model\n",
    "\n",
    "**Question 19:** Given your understanding of the task, which (very simple) loss function should we use in our trainer function? Show your updated code in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9546c6c-8a67-40de-aea4-156936326ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, num_epochs, learning_rate):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, outputs, mid in dataloader:\n",
    "            # Clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            pred = model(inputs, outputs, mid)\n",
    "            # Calculate loss\n",
    "            loss = criterion(pred, outputs)\n",
    "            total_loss += loss.item()\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Print total loss every few epochs\n",
    "        if epoch % 25 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {total_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efb97f-51b2-4bdf-8927-717d17be468f",
   "metadata": {},
   "source": [
    "Having figure out the correct models and trainer function, you may not use the celml below. It will train the model from scratch, on 50 iterations and will show you the amount of time take to train this model. This is just information to let you know how long the full training loop (in the next cell), might take on your machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83d036-543f-4970-892a-ea0e7d6ef807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "%timeit -r 1 -n 1 train(dataloader = pt_dataloader, model = seq2seq_model, num_epochs = 26, learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b1d43-94f9-48a2-b6d8-16810a7d5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was used to save a starting point for the next cell, do not run.\n",
    "#torch.save(seq2seq_model.state_dict(), 'seq2seq_model_start.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4454380-e706-47e1-8b19-6b51342b338a",
   "metadata": {},
   "source": [
    "**Question 20:** It seems the loss values we are seeing when using the model with randomly initialized parameters is very high. While it seems to decrease, it seems lots of iterations will be needed. The next cell suggests to run the training loop, but initialize the weights of the model using values in the *seq2seq_model_start.pth* file, presumably coming from another roughly similar model, trained on a different but similar task. This is done in an attempt to help the model train better and faster. Under which name is this concept known in Deep Learning?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note: The next cell will take a (somewhat) long time to run. On my machine CPU, it takes ~4minutes.<br>\n",
    "You can simply guess how long it will take to run on your machine, by using the execution time of the previous cell (using 50 iterations) and multiplying that by 20. As mentioned at the beginning of this Notebook, we have observed issues in running the code, on some CUDA machines. It is unclear at the moment, so try GPU computing at your own risk... </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c034a-03ba-46a1-b369-121f9756fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "# Start from given parameters to make training easier\n",
    "seq2seq_model.load_state_dict(torch.load('seq2seq_model_start.pth'))\n",
    "%timeit -r 1 -n 1 train(dataloader = pt_dataloader, model = seq2seq_model, num_epochs = 1051, learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa2893-acc6-4985-8556-14a5acfe403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not uncomment and execute, this was used to prepare the model that you will be using in the final question!\n",
    "#torch.save(seq2seq_model.state_dict(), 'seq2seq_model_end.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb861c49-f721-410d-b71a-56f0a8dc18ec",
   "metadata": {},
   "source": [
    "**Question 21:** The code below shows the predictions produced by your Seq2Seq model after training and can be used to confirm that you have trained the right model! Show some screenshots in your report, and discuss the final performance you have obtained for your model. For your information, I typically obtain an MSE of ~0.05 after 1000 iterations of training. Additional performance can probably be obtained via hyperparameters tuning (changing the size of memory vector, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facc4e8-e27b-4844-881f-20b48daff13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on our Seq2Seq model\n",
    "# (Seeding for reproducibility)\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "seq2seq_model.load_state_dict(torch.load('seq2seq_model_end.pth'))\n",
    "seed_value = 187\n",
    "test_model(seq2seq_model, pt_dataloader, seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e87e31-7dd3-4af4-9c5e-044fbde05567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "# (Seeding for reproducibility)\n",
    "hidden_size = 25\n",
    "seq2seq_model = Seq2Seq(input_size = n_inputs, \\\n",
    "                        hidden_size = hidden_size, \\\n",
    "                        output_size = n_outputs).to(device)\n",
    "seq2seq_model.load_state_dict(torch.load('seq2seq_model_end.pth'))\n",
    "# Visualize\n",
    "visualize_some_predictions(seq2seq_model, pt_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442801e-e21e-4721-8e13-b789c14ddca5",
   "metadata": {},
   "source": [
    "## This concludes HW3.\n",
    "\n",
    "Do not give up, it is a feasible task! If your model does not work, most likely, you are making a mistake in the Encoder model or - most likely - the Decoder model. Take your time to think about the task at hand and the model we should use for that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e0296-cd6f-40f5-a4d1-a3e88ec77b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
