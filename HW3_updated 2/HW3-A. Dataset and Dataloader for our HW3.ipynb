{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1afe191-5213-431c-8f47-a0502c1670c5",
   "metadata": {},
   "source": [
    "# HW3-A. Dataset and Dataloader for our HW3\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (06/03/2024)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3\n",
    "- Matplotlib\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Torch\n",
    "- Torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9dd5db-658b-4b9c-8e8b-25f6710a791b",
   "metadata": {},
   "source": [
    "## 0. Imports and CUDA\n",
    "\n",
    "In addition to the libraries mentioned above, you will need the *helper_functions.py* file, which contains a few additional functions that help make this notebook simpler for you (e.g. visualisation, test cases, etc.)\n",
    "\n",
    "Please refrain from modifying said file, but feel free to have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4599aff-3fcc-42ef-b3c1-512ac2ed80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Helper functions (additional file)\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066fafb-d4f6-4409-a976-6f1d9697fefa",
   "metadata": {},
   "source": [
    "While not necessary, you might want to run the code for this homework using GPU. It remains possible, however, to use CPU only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604b237-ac58-4c0d-a041-1dcb704972e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a049e-ea5a-43db-afac-b32600d0bb57",
   "metadata": {},
   "source": [
    "## 0. Dataset Excel file generation\n",
    "\n",
    "This section was used to generate the dataset. You may choose to uncomment code and run this cell again if you please, it is normally seeded and should not change the values in the dataset.xlsx file. Shall that happen for a reason we did not anticipate, you may simply download the dataset file again.\n",
    "\n",
    "If you look at the function create_dataset() in the helper_functions.py file, you will recognize that we have generate 4120 values, by using a [Stochastic Process]('https://en.wikipedia.org/wiki/Stochastic_process') of some sort.\n",
    "More specifically, all the entries of this time series are defined as:\n",
    "\n",
    "$$ x(t+1) = x(t) \\times (1 + a) + x(t) \\times b \\times n(t) $$\n",
    "\n",
    "$$ \\text{With, } x(0) = x_0, \\text{ known, and }  n(t) \\rightarrow N(0, 1) $$\n",
    "\n",
    "The values, used for $ x_0 $, $ a $ and $ b $ are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c3dfe-1838-4566-aa0c-fe590c3614f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was used to generate a dataset with 4120 values in a time series.\n",
    "x0 = 10\n",
    "a = 5e-4\n",
    "b = 5e-3\n",
    "n_points = 4120\n",
    "np.random.seed(27)\n",
    "#times, values = create_dataset(n_points, a, b, x0)\n",
    "#print(times.shape)\n",
    "#print(values.shape)\n",
    "#save_dataset(times, values, excel_file_path = 'dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1f34e-653c-4304-8b8a-9ab3d1d62825",
   "metadata": {},
   "source": [
    "## 1. Loading and visualizing the dataset\n",
    "\n",
    "In this first section, we are going to load the dataset from the *'dataset.xlsx'* file and explore said dataset. Feel free to have a look at this Excel file if you need.\n",
    "\n",
    "The cells below will define the parameters of our dataset, and load the data from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e97e2e-b953-460d-856c-c9f2bce0e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from file\n",
    "excel_file_path = 'dataset.xlsx'\n",
    "times, values = load_dataset(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a9a0a-cc7b-4595-a58a-fd5a52a73925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data in arrays\n",
    "print(times.shape, values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabff1d-b148-4494-85b3-25f6076bb7cb",
   "metadata": {},
   "source": [
    "The visualization below shows the samples in the dataset. As you can see, it consists of a time series of some sort, with 4120 time entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fa03d-d70d-4667-812b-a59c1e9ffbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plot_dataset(times, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1449e-7bbb-4656-b9a0-499b1fb00d65",
   "metadata": {},
   "source": [
    "Let us pretend that this dataset, and its curve shown above, consist of values observed on the stock market over time.\n",
    "\n",
    "Using stochastic processes to model the stock market, such as the one described in our function above, captures the essence of market volatility and randomness. Such models, incorporating randomness, allow for a range of possible outcomes, making them quite suitable for financial market simulations where uncertainty is a constant feature. In fact, one could argue that the curve above is somewhat representative of the typical curves the stock market would produce.\n",
    "\n",
    "However, there are limitations to relying solely on stochastic processes for stock market modeling.\n",
    "\n",
    "One significant limitation is the blatant simplification of reality of such synthetic datasets. Stochastic models are built on assumptions that may not fully capture the complexities of the real-world market. For instance, our dataset assumes that market conditions remain constant over time or follow a predictable pattern, which is rarely the case in volatile markets. More often than not, the values of the stochastic process coefficients ($ a $ and $ b $) should vary over time, instead of remaining constant.\n",
    "\n",
    "Another signification limitation to our dataset, is that the noise level, defined by coefficient $ b $, is far below what real-life volatile markets would usually require. This large presence of noise in the data, in turn, makes it very difficult for ML models to accurately and reliably predict the market.\n",
    "\n",
    "In this HW, we will attempt to design an AI, which \n",
    "- receives as inputs a sequence of 20 values: $ x(t), x(t+1), ..., x(t+20) $,\n",
    "- and attempts to predict the next 5 values to be produced by the market, i.e. $ x(t+20+1), x(t+20+2), ..., x(t+20+5) $.\n",
    "\n",
    "**Question 0:** This question is worth no points and serves as a disclaimer. The HW we have prepared here can not be easily transposed and reused to real-life. The quest to design an AI that is capable of predicting the market is a difficult one, and is currently seen as the holy grail of finance. The notions presented in this HW are shown to you, for educational purposes only. Shall you decide to design an AI following ideas discussed in this HW and play with the stock market, SUTD (and yours truly) cannot be held responsible for any losses incurred by the use of this AI on the stock market (seriously, don't).\n",
    "\n",
    "**Question 1:** Knowing full well of the limitations of the synthetic dataset you are about to play with, do you think the AI we will create and whose job will be to predict the next values of the market will generalize well to real-life markets? Which principle seen in class during W1-W3 explains this generalization issue? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4e25c-cb04-4ea6-8a72-de4e92fa9b50",
   "metadata": {},
   "source": [
    "## 2. Writing a PyTorch Dataset object\n",
    "\n",
    "Right now, we would like to write a *PyTorch Dataset* object for our Machine Learning problem.\n",
    "\n",
    "Have a look at the incomplete code below, you will recognize that there are several None variables (called *None1*, *None2*, etc.). These variables probably need to be replaced with something else.\n",
    "\n",
    "Your class is expected to have the following features.\n",
    "- Initialization (__init__ method): The dataset initializes by reading an Excel file (dataset.xlsx) using Pandas read_excel function and stores it in the dataframe attribute.\n",
    "- Length method (__len__ method): This method should return a certain information about the dataset.\n",
    "- Get item method (__getitem__ method): This method is called when you index into the dataset (e.g., dataset[idx]).\n",
    "- There will be an additional method, define_samples(), which will break down the data in the dataset into appropriate chunks.\n",
    "\n",
    "**Question 2:** Study the code for the Dataset below. How many tensors will be returned when the dataset is summoned using and index *t*, by using the operation *dataset[t]* for instance? What will be the sizes of these tensors? If needed, feel free to play with the Dataset object a bit to find your answer. More importantly, what is the information contained in each of these tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b872fc-9a4a-4058-9c52-958423e5c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, times, values, n_inputs, n_outputs):\n",
    "        self.times = times\n",
    "        self.values = values\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.define_samples()\n",
    "\n",
    "    def define_samples(self):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.mid = []\n",
    "        # Define all inputs\n",
    "        for i in range(len(times) - self.n_inputs - self.n_outputs + 1):\n",
    "            # Last input not included\n",
    "            next_input = self.values[i:(i + self.n_inputs - 1)]\n",
    "            # Output\n",
    "            next_output = self.values[(i + self.n_inputs):(i + self.n_inputs + self.n_outputs)]\n",
    "            # Mid is the turning point, i.e. the value of the last sample in the series of inputs\n",
    "            next_mid = [self.values[i + self.n_inputs - 1]]\n",
    "            self.inputs.append(next_input)\n",
    "            self.outputs.append(next_output)\n",
    "            self.mid.append(next_mid)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Select samples corresponding to the different inputs\n",
    "        # and outputs we have created with the define_samples() function,\n",
    "        # and convert them to PyTorch tensors\n",
    "        x = torch.tensor(self.inputs[idx], dtype = torch.float32)\n",
    "        y = torch.tensor(self.outputs[idx], dtype = torch.float32)\n",
    "        m = torch.tensor(self.mid[idx], dtype = torch.float32)\n",
    "        return x, y, m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d568e-3e2e-4221-9dbb-92d51d08bbdd",
   "metadata": {},
   "source": [
    "We will now create our dataset object, using the pair of values for *n_inputs = 20* and *n_outputs = 5*.\n",
    "\n",
    "**Question 3:** For this combination of values, how many samples will the dataset contain? You may need to ask your dataset object using a certain operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca3f81-6a13-4d38-adb0-c849f19abf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our PyTorch Dataset object from the class above\n",
    "n_inputs = 20\n",
    "n_outputs = 5\n",
    "pt_dataset = CustomDataset(times, values, n_inputs, n_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010b4e5-fe8c-424c-876f-90d1216f3e1d",
   "metadata": {},
   "source": [
    "The cell below can be used to visualize the values stored in the tensor with index *idx*. Feel free to experiment with the value of idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f91da-71d9-4feb-a688-da391bf44d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our dataset object and visualizing data\n",
    "t = 32\n",
    "inputs, outputs, mid = pt_dataset[t]\n",
    "visualize_samples(inputs, mid, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a0b75-f383-4735-832d-fdf16b944cf1",
   "metadata": {},
   "source": [
    "## 3. Writing a Dataloader object\n",
    "\n",
    "Our next task is now to write a PyTorch dataloader object. It will serve as a conveyor belt for our PyTorch dataset object defined in the previous section.\n",
    "\n",
    "Its objective will be to form mini-batches of size 256.\n",
    "\n",
    "**Question 4:** In the code below, there is a *True* value being assigned to the *shuffle* parameter of the dataloader. Until now, we have always used the *shuffle = True* configuration. Would it make sense to change it to a False for the task at hand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088786e-8769-42be-a275-0bed23d69372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99555644-6369-4f33-8040-9336594846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader object\n",
    "pt_dataloader = DataLoader(pt_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14a6a6-e83b-4966-b129-02029619001c",
   "metadata": {},
   "source": [
    "**Question 5:** If you run the code below, you will get to see the value 16. What does this value 16 correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dc92e-6a55-4fff-8e2b-05b10953a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our dataloader object\n",
    "print(len(pt_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214cd9f-8c25-40b4-9730-e89e29a11fab",
   "metadata": {},
   "source": [
    "## What is next?\n",
    "\n",
    "Our task continues in the Notebook 3-B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160271e-96db-4197-8e37-7d4baeff6878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
