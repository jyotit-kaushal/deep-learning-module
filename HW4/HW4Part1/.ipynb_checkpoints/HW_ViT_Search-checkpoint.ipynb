{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reverse-cathedral",
   "metadata": {},
   "source": [
    "# # HW4 - A Guided Homework on Proxies for Estimating Ranking of Vision Transformer Architectures\n",
    " \n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.7.16)\n",
    "- numpy==1.21.5\n",
    "- torch==1.13.1\n",
    "- torchvision==0.14.1\n",
    "- timm==0.4.12\n",
    "- opencv-python==4.9.0.80\n",
    "- scipy==1.7.3\n",
    "- scikit-image==0.19.2\n",
    "- pyyaml==5.4.1\n",
    "- easydict==1.13\n",
    "- matplotlib\n",
    "- ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-failure",
   "metadata": {},
   "source": [
    "### 0. Prelim: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from timm.utils.model import unwrap_model\n",
    "from lib.datasets import build_dataset\n",
    "from lib import utils\n",
    "import json\n",
    "from scipy import stats\n",
    "import json\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6ce4",
   "metadata": {},
   "source": [
    "### 1. Prelim: Hyperparameters and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277741d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "api_Auto_FM_Benchmark='./AutoFM_CVPR2022_API_5_7M.json'\n",
    "input_size=224\n",
    "data_path='./dataset/imagenet'              \n",
    "seed=0\n",
    "num_workers=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is cpu-friendly, but running the notebook with GPU(s) will drastically speed up the computation.\n",
    "\n",
    "# Define device for torch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed9097",
   "metadata": {},
   "source": [
    "### 2. Prelim: Dataset and Dataloader\n",
    "\n",
    "This section loads data into PyTorch Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed the seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Enables automatic selection of the most efficient algorithms for deep learning operations\n",
    "cudnn.benchmark = True\n",
    "\n",
    "### Build the dataloader ###\n",
    "dataset_val, nb_classes = build_dataset(True, data_path, input_size, \"train\")  # load data of one batch\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "    \n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size,\n",
    "    sampler=sampler_val, num_workers=num_workers,\n",
    "    pin_memory=True, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d68cf",
   "metadata": {},
   "source": [
    "### 3. Load Vision Transformer architectures\n",
    "In this homework, you will write proxies to predict the ranking of Vision Transformer architectures. In particular, you will compute the proxy scores based on the initialized weights, using one forward and backward pass of one minibatch of samples. No training is performed on the Vision Transformers to predict the ranking. \n",
    "\n",
    "We have already prepared 50 Vision Transformer architectures for this homework. For evaluation of the performance of the proxies in ranking, we also provide test accuracy for these architectures. Note that the test accuracy is not used in the proxies. The test accuracy is used only to evaluate the effectiveness of the proxy scores for ranking. \n",
    "\n",
    "First, we load 50 architectures into arch_candidate_set. For each architecture, following information is included:\n",
    "\n",
    "1) the architectural configuration;\n",
    "\n",
    "2) the test accuracy (for image classification) on ImageNet-1k validation set. This information is not used in the proxies. This test accuracy is for evaluating the proxies in architecture ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adaf02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load candidate architectures\n",
    "from model.get_Vision_Transformer_Arch import model_VIT\n",
    "file_api = open(api_Auto_FM_Benchmark)\n",
    "arch_candidate_set =json.load(file_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b9fa4",
   "metadata": {},
   "source": [
    "### 4. Develop functions to compute Proxies for Ranking Vision Transformer Architectures\n",
    "\n",
    "This section studies some proxies that estimate the ranking of Vision Transformer architectures, based on only the initialized weights and the associated gradients in one forward and backward pass.\n",
    "\n",
    "**Learnable parameters**: For Vision Transformers [1], the learnable parameters are mainly from linear layers (denoted by 'nn.linear' in PyTorch) of different components (self-attention, multilayer perceptron, etc). Particularly, in this exercise, the patch embeddings in the Vision Transformers are obtained via 2-D convolutions instead of linear projection of flattened patches (details omitted). \n",
    "\n",
    "In summary, the learnable parameters can be found in 'nn.Linear' and 'nn.Conv2d' modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626981ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_net_proxy(net, metric):\n",
    "    metric_array = []\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return sum(metric_array).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a15c3",
   "metadata": {},
   "source": [
    "**Question 1**: Write your proxies: grad_norm and SNIP\n",
    "\n",
    "The goal is to design a proxy (function) that can predict the ranking Vision Transformer architectures, without training the networks [3]. We will consider some simple proxies:\n",
    "\n",
    "**Gradient Norm (grad_norm)**: This proxy computes the Euclidean norm of the gradients by layer [3] at network initialization. A large grad_norm could indicate more significant coefficients in the network, suggesting a better network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b11b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm_scores(net, inputs, targets, loss_fn):\n",
    "\n",
    "  grad_norm_arr = 0\n",
    "  ### TODO: Complete the function ###\n",
    "\n",
    "  return grad_norm_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612c510",
   "metadata": {},
   "source": [
    "**SNIP**: This proxy sums up the absolute values of all the weights multiplied by their gradients [3,4] at initialization. A large SNIP could indicate a better network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SNIP_scores(net, inputs, targets, loss_fn):\n",
    "\n",
    "  SNIP_arr = 0\n",
    "  ### TODO: Complete the function ###\n",
    "\n",
    "  return SNIP_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c07b6",
   "metadata": {},
   "source": [
    "### 5. Using the Proxies to estimate the ranking of Vision Transformer Architectures\n",
    "This section computes the proxy scores for the Vision Transformer architectures in arch_candidate_set, which were loaded in step 3.\n",
    "\n",
    "**Leveraging Vision Transformer architectures' information**: We will utilize the information for each architecture in arch_candidate_set: \n",
    "\n",
    "- **Architecture configuration**: This allows us to create the corresponding PyTorch model representing the specific Vision Transformer architecture. Then, we apply our proxies on the model.\n",
    "- **Test accuracy**: The provided test accuracy will be used to evalute the ranking obtained by our proxy scores. \n",
    "\n",
    "**Proxy Computation**: We will employ the proxies: **grad_norm**, **SNIP** (defined earlier) to compute proxy scores for each Vision Transformer architecture in arch_candidate_set. The proxy scores predict the ranking of the Vision Transformer architectures without actually training them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c0415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### get data sample for proxy computation ###\n",
    "x, y = next(iter(data_loader_val))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# specify the loss function for proxy computation\n",
    "lossfunc = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# specify the proxy\n",
    "proxy = 'grad_norm' \n",
    "\n",
    "# get 50 architectures and compute the proxy scores\n",
    "proxy_scores = []\n",
    "accs = []\n",
    "st_time = time.time()\n",
    "\n",
    "for arch_idx, arch_info in arch_candidate_set.items():\n",
    "    if int(arch_idx) == 50:\n",
    "        break\n",
    "    e_time = time.time()\n",
    "    \n",
    "    # get network information (configuration, test accuracy) from API\n",
    "    net_setting = arch_info['net_setting']\n",
    "    net_arch = unwrap_model(model_VIT)\n",
    "    net_arch.set_sample_config(config=net_setting)\n",
    "    net_arch.to(device)\n",
    "\n",
    "    # compute for the proxy\n",
    "    if proxy == 'grad_norm':\n",
    "        res = get_grad_norm_scores(net_arch, x, y,lossfunc)\n",
    "    elif proxy =='snip':\n",
    "        res = get_SNIP_scores(net_arch,x, y, lossfunc)\n",
    "\n",
    "    # Store the results for computing the correlation between the proxy and test accuracy\n",
    "    del net_arch\n",
    "    print('Architectures: ',arch_idx)\n",
    "    print('Test-Accuracy: ', arch_info['test-accuracy'])\n",
    "    proxy_scores.append(res)\n",
    "    accs.append(arch_info['test-accuracy'])\n",
    "    print('Proxy score: ',res)\n",
    "    edl_time = time.time()\n",
    "    print('Computation Time: ',edl_time-e_time)   \n",
    "    print('---------------------------------------------')\n",
    "end_time = time.time()\n",
    "print('total time: ',end_time-st_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8fafa",
   "metadata": {},
   "source": [
    "### 6. Evaluating Proxy Effectiveness\n",
    "This section focuses on evaluating the effectiveness of the proxy in predicting the ranking of architectures.\n",
    "\n",
    "**Correlation Analysis**: We will calculate Kendall correlation between the ranking based on the proxy scores and the ranking based on the provided test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8604dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation with accuracy\n",
    "kendalltau = stats.kendalltau(proxy_scores, accs)\n",
    "print('*'*50)\n",
    "print('Kendalltau:', kendalltau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d18f2",
   "metadata": {},
   "source": [
    "### 7. Visualizing the Correlation Distribution\n",
    "This section visualizes the correlation between the proxy scores and the test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ded2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=proxy_scores, y=accs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Proxy Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920c397",
   "metadata": {},
   "source": [
    "### 8. Identifying Top Architectures based on Proxies\n",
    "This section focuses on identifying the top-performing architectures based on the computed proxy scores.\n",
    "\n",
    "**Ranking by Proxy Scores*: We will sort the architectures in descending order based on their proxy scores. This ranking prioritizes architectures with higher predicted performance.\n",
    "\n",
    "**Top Architectures**: We can then identify the top-ranked architectures (e.g., top 1 or top 5) as potential candidates for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df163d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.get_subnet_Vision_Transformer_Arch import get_subnet_arch\n",
    "\n",
    "# Get the index of top-1 architecture based on the proxy scores\n",
    "best_index_architectures = np.argmax(proxy_scores)\n",
    "\n",
    "# Load the top-1 architecture and check the layer-wise details\n",
    "net_arch = arch_candidate_set[str(best_index_architectures)]['net_setting']\n",
    "model_best_by_proxys = get_subnet_arch(net_arch)\n",
    "\n",
    "\n",
    "print(model_best_by_proxys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2cf1f",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[1] Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020).\n",
    "\n",
    "[2] Chen, Minghao, Houwen Peng, Jianlong Fu, and Haibin Ling. \"Autoformer: Searching transformers for visual recognition.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12270-12280. 2021.\n",
    "\n",
    "[3] Li, Guihong, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, and Radu Marculescu. \"Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities.\" arXiv preprint arXiv:2307.01998 (2023).\n",
    "\n",
    "[4] Lee, Namhoon, Thalaiyasingam Ajanthan, and Philip HS Torr. \"Snip: Single-shot network pruning based on connection sensitivity.\" arXiv preprint arXiv:1810.02340 (2018)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoformerassign",
   "language": "python",
   "name": "autoformerassign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
