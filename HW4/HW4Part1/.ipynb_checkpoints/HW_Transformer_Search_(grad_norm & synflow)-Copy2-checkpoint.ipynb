{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reverse-cathedral",
   "metadata": {},
   "source": [
    "# # HW - A Guided Tutorial on Proxies Estimating Performance of Vision Transformers\n",
    "**Author:** \n",
    "**Version:** \n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.7.16)\n",
    "- numpy==1.21.5\n",
    "- torch==1.13.1\n",
    "- torchvision==0.14.1\n",
    "- timm==0.4.12\n",
    "- opencv-python==4.9.0.80\n",
    "- scipy==1.7.3\n",
    "- scikit-image==0.19.2\n",
    "- pyyaml==5.4.1\n",
    "- easydict==1.13\n",
    "- matplotlib\n",
    "- ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-failure",
   "metadata": {},
   "source": [
    "### 0. Prelim: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boxed-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from timm.utils.model import unwrap_model\n",
    "from lib.datasets import build_dataset\n",
    "from lib import utils\n",
    "import json\n",
    "from scipy import stats\n",
    "import json\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6ce4",
   "metadata": {},
   "source": [
    "### 1. Prelim: Hyperparameters and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277741d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "api_Auto_FM_Benchmark='./AutoFM_CVPR2022_API_5_7M.json'\n",
    "input_size=224\n",
    "data_path='./dataset/imagenet'              \n",
    "seed=0\n",
    "num_workers=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b36d1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "# The code is cpu-friendly, but running the notebook with GPU(s) will drastically speed up the computation.\n",
    "\n",
    "# Define device for torch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed9097",
   "metadata": {},
   "source": [
    "### 2. Prelim: Dataset and Dataloader\n",
    "\n",
    "This section loads data into PyTorch Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba04c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "# Fixed the seed for reproducibility\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True  # enables automatic selection of the most efficient algorithms for deep learning operations\n",
    "\n",
    "##load data\n",
    "dataset_val, nb_classes = build_dataset(True, data_path,input_size, \"train\")  # load data of one batch\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "    \n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size,\n",
    "    sampler=sampler_val, num_workers=num_workers,\n",
    "    pin_memory=True, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d68cf",
   "metadata": {},
   "source": [
    "### 3. Use AutoFormer API to introduce candidate architectures\n",
    "This section loads the candidate architectures via an external API [1] and use it to explore Vision Transformers from an architectural perspective. 50 architectures are introduced. For each architecture, the API will provide:\n",
    "\n",
    "1) the architectural configuration;\n",
    "\n",
    "2) the test accuracy (for image classification) on ImageNet-1k validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adaf02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "### load candidate architectures\n",
    "from model.get_Vision_Transformer_Arch import model_VIT\n",
    "file_api = open(api_Auto_FM_Benchmark)\n",
    "arch_candidate_set =json.load(file_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b9fa4",
   "metadata": {},
   "source": [
    "### 4. Develop functions to compute Proxies for Architecture Performance Estimation\n",
    "\n",
    "This section focuses on the two proxies that estimate the performance of candidate Vision Transformer architectures\n",
    "\n",
    "**Question 2**: Write your proxy\n",
    "\n",
    "The goal is to design a proxy (function) that can predict the performance of some given networks without training the network parameters [2]. We will consider two simple proxies:\n",
    "\n",
    "**SNIP**: This proxy measures the each weight multiplied by its gradient [2,3] at initialization.\n",
    "\n",
    "**Gradient Norm (grad_norm)**: This proxy measures the norm of the gradients by layer [2] at initialization. A lower gradient norm may signify training difficulties, though a high norm could indicate the problem of exploding gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b11b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_metric_array_grad_norm(net, metric):\n",
    "    metric_array = []\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return sum(metric_array).item()\n",
    "\n",
    "def get_grad_norm_scores(net, inputs, targets, loss_fn):\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    net.zero_grad()\n",
    "    outputs = net.forward(inputs)\n",
    "    \n",
    "    loss = loss_fn(outputs, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norm_arr = get_layer_metric_array_grad_norm(net, lambda l: l.weight.grad.norm() if l.weight.grad is not None else torch.zeros_like(l.weight))\n",
    "    \n",
    "    return grad_norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5d18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_metric_array_synflow(net, metric):\n",
    "    metric_array = []\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return metric_array\n",
    "\n",
    "def compute_synflow_per_weight(net, inputs, targets, loss_fn):\n",
    "\n",
    "    device = inputs.device\n",
    "\n",
    "    #convert params to their abs. Keep sign for converting it back.\n",
    "    @torch.no_grad()\n",
    "    def linearize(net):\n",
    "        signs = {}\n",
    "        for name, param in net.state_dict().items():\n",
    "            signs[name] = torch.sign(param)\n",
    "            param.abs_()\n",
    "        return signs\n",
    "\n",
    "    #convert to orig values\n",
    "    @torch.no_grad()\n",
    "    def nonlinearize(net, signs):\n",
    "        for name, param in net.state_dict().items():\n",
    "            if 'weight_mask' not in name:\n",
    "                param.mul_(signs[name])\n",
    "\n",
    "    # keep signs of all params\n",
    "    signs = linearize(net)\n",
    "    \n",
    "    # Compute gradients with input of 1s \n",
    "    net.zero_grad()\n",
    "    net.float()\n",
    "    outputs = net.forward(inputs)\n",
    "    \n",
    "    loss = loss_fn(outputs, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # select the gradients that we want to use for search/prune\n",
    "    def synflow(layer):\n",
    "        if layer.weight.grad is not None:\n",
    "            return torch.abs(layer.weight * layer.weight.grad)\n",
    "        else:\n",
    "            return torch.zeros_like(layer.weight)\n",
    "    ## computed synflow score for each layer\n",
    "    grads_abs = get_layer_metric_array_synflow(net, synflow)\n",
    "\n",
    "    ## sum synflow score for all layer\n",
    "    def sum_arr(arr):\n",
    "        sum = 0.\n",
    "        for i in range(len(arr)):\n",
    "            sum += torch.sum(arr[i])\n",
    "        return sum.item()\n",
    "\n",
    "    grads_abs = sum_arr(grads_abs)\n",
    "    # apply signs of all params\n",
    "    nonlinearize(net, signs)\n",
    "\n",
    "    return grads_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c07b6",
   "metadata": {},
   "source": [
    "### 5. Computing Proxies for Estimating Performance of Candidate Architectures\n",
    "This section computes the proxy scores for the candidate Vision Transformer architectures from the API.\n",
    "\n",
    "**Leveraging Vision Transformer architectures candidates information**: We will utilize the information retrieved from the API for each Vision Transformer architecture:\n",
    "\n",
    "- **Architecture configuration**: This allows us to create the corresponding PyTorch model representing the specific Vision Transformer architectures candidates.\n",
    "- **Test accuracy**: While the API might provide test accuracy, we won't rely on it directly for ranking here.\n",
    "\n",
    "**Proxy Computation**: We will employ the proxy function: **grad_norm** and **SynFlow** (defined earlier) to compute a score for each Vision Transformer architectures candidate. This score estimates the potential performance of the corresponding Vision Transformer architecture without actually training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23c0415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 16, 16])\n",
      "tensor(19.1641, grad_fn=<SumBackward0>)\n",
      "tensor(0.3071, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7682, grad_fn=<SumBackward0>)\n",
      "tensor(6.4485e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4427, grad_fn=<SumBackward0>)\n",
      "tensor(0.0166, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.7797, grad_fn=<SumBackward0>)\n",
      "tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.7506, grad_fn=<SumBackward0>)\n",
      "tensor(0.0213, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9732, grad_fn=<SumBackward0>)\n",
      "tensor(7.5520e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.7208, grad_fn=<SumBackward0>)\n",
      "tensor(0.0245, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2248, grad_fn=<SumBackward0>)\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2349, grad_fn=<SumBackward0>)\n",
      "tensor(0.0167, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6773, grad_fn=<SumBackward0>)\n",
      "tensor(2.2303e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4500, grad_fn=<SumBackward0>)\n",
      "tensor(0.0229, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3707, grad_fn=<SumBackward0>)\n",
      "tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4180, grad_fn=<SumBackward0>)\n",
      "tensor(0.0202, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6771, grad_fn=<SumBackward0>)\n",
      "tensor(3.2823e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1591, grad_fn=<SumBackward0>)\n",
      "tensor(0.0159, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3248, grad_fn=<SumBackward0>)\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3280, grad_fn=<SumBackward0>)\n",
      "tensor(0.0180, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8314, grad_fn=<SumBackward0>)\n",
      "tensor(5.8136e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5408, grad_fn=<SumBackward0>)\n",
      "tensor(0.0182, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4612, grad_fn=<SumBackward0>)\n",
      "tensor(0.0020, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4780, grad_fn=<SumBackward0>)\n",
      "tensor(0.0205, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9032, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4582, grad_fn=<SumBackward0>)\n",
      "tensor(0.0185, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2876, grad_fn=<SumBackward0>)\n",
      "tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2348, grad_fn=<SumBackward0>)\n",
      "tensor(0.0144, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4534, grad_fn=<SumBackward0>)\n",
      "tensor(2.6462e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.8373, grad_fn=<SumBackward0>)\n",
      "tensor(0.0101, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5200, grad_fn=<SumBackward0>)\n",
      "tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4962, grad_fn=<SumBackward0>)\n",
      "tensor(0.0184, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8874, grad_fn=<SumBackward0>)\n",
      "tensor(8.3894e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.3816, grad_fn=<SumBackward0>)\n",
      "tensor(0.0148, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4645, grad_fn=<SumBackward0>)\n",
      "tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3668, grad_fn=<SumBackward0>)\n",
      "tensor(0.0121, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4948, grad_fn=<SumBackward0>)\n",
      "tensor(2.5728e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.9433, grad_fn=<SumBackward0>)\n",
      "tensor(0.0078, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1753, grad_fn=<SumBackward0>)\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1331, grad_fn=<SumBackward0>)\n",
      "tensor(0.0104, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.3797, grad_fn=<SumBackward0>)\n",
      "tensor(8.9946e-06, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7258, grad_fn=<SumBackward0>)\n",
      "tensor(0.0054, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3333, grad_fn=<SumBackward0>)\n",
      "tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3105, grad_fn=<SumBackward0>)\n",
      "tensor(0.0105, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7535, grad_fn=<SumBackward0>)\n",
      "tensor(9.0264e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5218, grad_fn=<SumBackward0>)\n",
      "tensor(0.0134, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2398, grad_fn=<SumBackward0>)\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2796, grad_fn=<SumBackward0>)\n",
      "tensor(0.0097, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.3965, grad_fn=<SumBackward0>)\n",
      "tensor(4.3006e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7290, grad_fn=<SumBackward0>)\n",
      "tensor(0.0047, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(0.9025, grad_fn=<SumBackward0>)\n",
      "tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.8848, grad_fn=<SumBackward0>)\n",
      "tensor(0.0060, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8084, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4475, grad_fn=<SumBackward0>)\n",
      "tensor(0.0106, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1180, grad_fn=<SumBackward0>)\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.0812, grad_fn=<SumBackward0>)\n",
      "tensor(0.0076, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7692, grad_fn=<SumBackward0>)\n",
      "tensor(9.8265e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4597, grad_fn=<SumBackward0>)\n",
      "tensor(0.0111, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1226, grad_fn=<SumBackward0>)\n",
      "tensor(0.0020, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1085, grad_fn=<SumBackward0>)\n",
      "tensor(0.0082, grad_fn=<SumBackward0>)\n",
      "torch.Size([1000, 256])\n",
      "tensor(1.1744, grad_fn=<SumBackward0>)\n",
      "tensor(0.5877, grad_fn=<SumBackward0>)\n",
      "Architectures:  0\n",
      "Test-Accuracy:  75.33999999755859\n",
      "Zerocost proxy score:  84.35889434814453\n",
      "Computation Proxy Time:  3.532724380493164\n",
      "---------------------------------------------\n",
      "torch.Size([256, 3, 16, 16])\n",
      "tensor(20.7310, grad_fn=<SumBackward0>)\n",
      "tensor(0.3206, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8284, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4999, grad_fn=<SumBackward0>)\n",
      "tensor(0.0175, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.8591, grad_fn=<SumBackward0>)\n",
      "tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.8497, grad_fn=<SumBackward0>)\n",
      "tensor(0.0224, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7757, grad_fn=<SumBackward0>)\n",
      "tensor(8.2777e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4065, grad_fn=<SumBackward0>)\n",
      "tensor(0.0219, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5085, grad_fn=<SumBackward0>)\n",
      "tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4442, grad_fn=<SumBackward0>)\n",
      "tensor(0.0227, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7064, grad_fn=<SumBackward0>)\n",
      "tensor(5.3690e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5170, grad_fn=<SumBackward0>)\n",
      "tensor(0.0267, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4378, grad_fn=<SumBackward0>)\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4537, grad_fn=<SumBackward0>)\n",
      "tensor(0.0224, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7074, grad_fn=<SumBackward0>)\n",
      "tensor(5.4856e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2367, grad_fn=<SumBackward0>)\n",
      "tensor(0.0181, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3982, grad_fn=<SumBackward0>)\n",
      "tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4010, grad_fn=<SumBackward0>)\n",
      "tensor(0.0199, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6514, grad_fn=<SumBackward0>)\n",
      "tensor(5.8948e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2330, grad_fn=<SumBackward0>)\n",
      "tensor(0.0163, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5709, grad_fn=<SumBackward0>)\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.5855, grad_fn=<SumBackward0>)\n",
      "tensor(0.0255, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7250, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2126, grad_fn=<SumBackward0>)\n",
      "tensor(0.0178, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3633, grad_fn=<SumBackward0>)\n",
      "tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3602, grad_fn=<SumBackward0>)\n",
      "tensor(0.0181, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6834, grad_fn=<SumBackward0>)\n",
      "tensor(1.5897e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1653, grad_fn=<SumBackward0>)\n",
      "tensor(0.0158, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(2.0019, grad_fn=<SumBackward0>)\n",
      "tensor(0.0018, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.8848, grad_fn=<SumBackward0>)\n",
      "tensor(0.0260, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9464, grad_fn=<SumBackward0>)\n",
      "tensor(8.9758e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4731, grad_fn=<SumBackward0>)\n",
      "tensor(0.0186, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5616, grad_fn=<SumBackward0>)\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4633, grad_fn=<SumBackward0>)\n",
      "tensor(0.0150, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5945, grad_fn=<SumBackward0>)\n",
      "tensor(2.7614e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1074, grad_fn=<SumBackward0>)\n",
      "tensor(0.0125, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(0.9658, grad_fn=<SumBackward0>)\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.9511, grad_fn=<SumBackward0>)\n",
      "tensor(0.0094, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8105, grad_fn=<SumBackward0>)\n",
      "tensor(1.0287e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4876, grad_fn=<SumBackward0>)\n",
      "tensor(0.0130, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4147, grad_fn=<SumBackward0>)\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4232, grad_fn=<SumBackward0>)\n",
      "tensor(0.0116, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4043, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7789, grad_fn=<SumBackward0>)\n",
      "tensor(0.0071, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5025, grad_fn=<SumBackward0>)\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.5549, grad_fn=<SumBackward0>)\n",
      "tensor(0.0125, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7066, grad_fn=<SumBackward0>)\n",
      "tensor(3.1630e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1747, grad_fn=<SumBackward0>)\n",
      "tensor(0.0075, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1974, grad_fn=<SumBackward0>)\n",
      "tensor(0.0035, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1764, grad_fn=<SumBackward0>)\n",
      "tensor(0.0082, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5948, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1018, grad_fn=<SumBackward0>)\n",
      "tensor(0.0082, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.0015, grad_fn=<SumBackward0>)\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.9767, grad_fn=<SumBackward0>)\n",
      "tensor(0.0068, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4418, grad_fn=<SumBackward0>)\n",
      "tensor(7.3333e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7935, grad_fn=<SumBackward0>)\n",
      "tensor(0.0058, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3573, grad_fn=<SumBackward0>)\n",
      "tensor(0.0026, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3242, grad_fn=<SumBackward0>)\n",
      "tensor(0.0099, grad_fn=<SumBackward0>)\n",
      "torch.Size([1000, 256])\n",
      "tensor(1.1882, grad_fn=<SumBackward0>)\n",
      "tensor(0.5994, grad_fn=<SumBackward0>)\n",
      "Architectures:  1\n",
      "Test-Accuracy:  75.21799999877929\n",
      "Zerocost proxy score:  88.67345428466797\n",
      "Computation Proxy Time:  3.4952139854431152\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 16, 16])\n",
      "tensor(20.4304, grad_fn=<SumBackward0>)\n",
      "tensor(0.2855, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7990, grad_fn=<SumBackward0>)\n",
      "tensor(9.7921e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4970, grad_fn=<SumBackward0>)\n",
      "tensor(0.0142, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5765, grad_fn=<SumBackward0>)\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.5774, grad_fn=<SumBackward0>)\n",
      "tensor(0.0159, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7506, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.3763, grad_fn=<SumBackward0>)\n",
      "tensor(0.0170, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.6591, grad_fn=<SumBackward0>)\n",
      "tensor(0.0018, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.5889, grad_fn=<SumBackward0>)\n",
      "tensor(0.0203, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(1.1361, grad_fn=<SumBackward0>)\n",
      "tensor(5.1697e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(2.1065, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.0373, grad_fn=<SumBackward0>)\n",
      "tensor(0.0016, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1235, grad_fn=<SumBackward0>)\n",
      "tensor(0.0150, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6951, grad_fn=<SumBackward0>)\n",
      "tensor(3.4050e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1865, grad_fn=<SumBackward0>)\n",
      "tensor(0.0152, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3999, grad_fn=<SumBackward0>)\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3977, grad_fn=<SumBackward0>)\n",
      "tensor(0.0188, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6737, grad_fn=<SumBackward0>)\n",
      "tensor(6.2817e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2001, grad_fn=<SumBackward0>)\n",
      "tensor(0.0146, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3743, grad_fn=<SumBackward0>)\n",
      "tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3487, grad_fn=<SumBackward0>)\n",
      "tensor(0.0213, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9680, grad_fn=<SumBackward0>)\n",
      "tensor(7.5516e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5568, grad_fn=<SumBackward0>)\n",
      "tensor(0.0211, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5244, grad_fn=<SumBackward0>)\n",
      "tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3979, grad_fn=<SumBackward0>)\n",
      "tensor(0.0168, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4958, grad_fn=<SumBackward0>)\n",
      "tensor(4.2281e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.9088, grad_fn=<SumBackward0>)\n",
      "tensor(0.0114, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3128, grad_fn=<SumBackward0>)\n",
      "tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2514, grad_fn=<SumBackward0>)\n",
      "tensor(0.0163, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4767, grad_fn=<SumBackward0>)\n",
      "tensor(8.2408e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.8449, grad_fn=<SumBackward0>)\n",
      "tensor(0.0090, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5437, grad_fn=<SumBackward0>)\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4729, grad_fn=<SumBackward0>)\n",
      "tensor(0.0143, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8315, grad_fn=<SumBackward0>)\n",
      "tensor(2.0854e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4750, grad_fn=<SumBackward0>)\n",
      "tensor(0.0136, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1901, grad_fn=<SumBackward0>)\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1440, grad_fn=<SumBackward0>)\n",
      "tensor(0.0112, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7615, grad_fn=<SumBackward0>)\n",
      "tensor(6.7908e-06, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4115, grad_fn=<SumBackward0>)\n",
      "tensor(0.0109, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1170, grad_fn=<SumBackward0>)\n",
      "tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.0818, grad_fn=<SumBackward0>)\n",
      "tensor(0.0080, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5939, grad_fn=<SumBackward0>)\n",
      "tensor(8.5954e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1422, grad_fn=<SumBackward0>)\n",
      "tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2494, grad_fn=<SumBackward0>)\n",
      "tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2362, grad_fn=<SumBackward0>)\n",
      "tensor(0.0089, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4433, grad_fn=<SumBackward0>)\n",
      "tensor(4.0256e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.8424, grad_fn=<SumBackward0>)\n",
      "tensor(0.0045, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3895, grad_fn=<SumBackward0>)\n",
      "tensor(0.0038, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3292, grad_fn=<SumBackward0>)\n",
      "tensor(0.0082, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8213, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4312, grad_fn=<SumBackward0>)\n",
      "tensor(0.0097, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1211, grad_fn=<SumBackward0>)\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1012, grad_fn=<SumBackward0>)\n",
      "tensor(0.0075, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6326, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1039, grad_fn=<SumBackward0>)\n",
      "tensor(0.0082, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.7201, grad_fn=<SumBackward0>)\n",
      "tensor(0.0031, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.6824, grad_fn=<SumBackward0>)\n",
      "tensor(0.0123, grad_fn=<SumBackward0>)\n",
      "torch.Size([1000, 256])\n",
      "tensor(1.2115, grad_fn=<SumBackward0>)\n",
      "tensor(0.6138, grad_fn=<SumBackward0>)\n",
      "Architectures:  2\n",
      "Test-Accuracy:  75.29799999755859\n",
      "Zerocost proxy score:  87.75236511230469\n",
      "Computation Proxy Time:  3.6096818447113037\n",
      "---------------------------------------------\n",
      "torch.Size([256, 3, 16, 16])\n",
      "tensor(20.6397, grad_fn=<SumBackward0>)\n",
      "tensor(0.2727, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8432, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5041, grad_fn=<SumBackward0>)\n",
      "tensor(0.0157, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.8473, grad_fn=<SumBackward0>)\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.8360, grad_fn=<SumBackward0>)\n",
      "tensor(0.0200, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7452, grad_fn=<SumBackward0>)\n",
      "tensor(7.8683e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4218, grad_fn=<SumBackward0>)\n",
      "tensor(0.0186, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5035, grad_fn=<SumBackward0>)\n",
      "tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4554, grad_fn=<SumBackward0>)\n",
      "tensor(0.0191, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9479, grad_fn=<SumBackward0>)\n",
      "tensor(2.9457e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.9728, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4369, grad_fn=<SumBackward0>)\n",
      "tensor(0.0022, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4774, grad_fn=<SumBackward0>)\n",
      "tensor(0.0194, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6953, grad_fn=<SumBackward0>)\n",
      "tensor(5.5253e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2313, grad_fn=<SumBackward0>)\n",
      "tensor(0.0160, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4202, grad_fn=<SumBackward0>)\n",
      "tensor(0.0017, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4200, grad_fn=<SumBackward0>)\n",
      "tensor(0.0181, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8621, grad_fn=<SumBackward0>)\n",
      "tensor(7.9395e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.6092, grad_fn=<SumBackward0>)\n",
      "tensor(0.0181, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2695, grad_fn=<SumBackward0>)\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3241, grad_fn=<SumBackward0>)\n",
      "tensor(0.0186, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7142, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1102, grad_fn=<SumBackward0>)\n",
      "tensor(0.0135, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3515, grad_fn=<SumBackward0>)\n",
      "tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3155, grad_fn=<SumBackward0>)\n",
      "tensor(0.0139, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5467, grad_fn=<SumBackward0>)\n",
      "tensor(3.1138e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.0841, grad_fn=<SumBackward0>)\n",
      "tensor(0.0109, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3477, grad_fn=<SumBackward0>)\n",
      "tensor(0.0018, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2990, grad_fn=<SumBackward0>)\n",
      "tensor(0.0158, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9434, grad_fn=<SumBackward0>)\n",
      "tensor(6.9951e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4591, grad_fn=<SumBackward0>)\n",
      "tensor(0.0142, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.0236, grad_fn=<SumBackward0>)\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.9974, grad_fn=<SumBackward0>)\n",
      "tensor(0.0081, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6011, grad_fn=<SumBackward0>)\n",
      "tensor(4.3430e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.0835, grad_fn=<SumBackward0>)\n",
      "tensor(0.0103, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(0.8996, grad_fn=<SumBackward0>)\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.9357, grad_fn=<SumBackward0>)\n",
      "tensor(0.0073, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4401, grad_fn=<SumBackward0>)\n",
      "tensor(1.1610e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.8477, grad_fn=<SumBackward0>)\n",
      "tensor(0.0058, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4176, grad_fn=<SumBackward0>)\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4261, grad_fn=<SumBackward0>)\n",
      "tensor(0.0095, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8130, grad_fn=<SumBackward0>)\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5531, grad_fn=<SumBackward0>)\n",
      "tensor(0.0117, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4925, grad_fn=<SumBackward0>)\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.5084, grad_fn=<SumBackward0>)\n",
      "tensor(0.0099, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8273, grad_fn=<SumBackward0>)\n",
      "tensor(3.0356e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4302, grad_fn=<SumBackward0>)\n",
      "tensor(0.0088, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1737, grad_fn=<SumBackward0>)\n",
      "tensor(0.0037, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1636, grad_fn=<SumBackward0>)\n",
      "tensor(0.0079, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8046, grad_fn=<SumBackward0>)\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.3952, grad_fn=<SumBackward0>)\n",
      "tensor(0.0102, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1039, grad_fn=<SumBackward0>)\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.0580, grad_fn=<SumBackward0>)\n",
      "tensor(0.0073, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7992, grad_fn=<SumBackward0>)\n",
      "tensor(9.3311e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4792, grad_fn=<SumBackward0>)\n",
      "tensor(0.0112, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3482, grad_fn=<SumBackward0>)\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3454, grad_fn=<SumBackward0>)\n",
      "tensor(0.0098, grad_fn=<SumBackward0>)\n",
      "torch.Size([1000, 256])\n",
      "tensor(1.1876, grad_fn=<SumBackward0>)\n",
      "tensor(0.5934, grad_fn=<SumBackward0>)\n",
      "Architectures:  3\n",
      "Test-Accuracy:  75.3259999987793\n",
      "Zerocost proxy score:  88.78964233398438\n",
      "Computation Proxy Time:  4.052223443984985\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 16, 16])\n",
      "tensor(19.3444, grad_fn=<SumBackward0>)\n",
      "tensor(0.2667, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7383, grad_fn=<SumBackward0>)\n",
      "tensor(4.1993e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4556, grad_fn=<SumBackward0>)\n",
      "tensor(0.0167, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.7726, grad_fn=<SumBackward0>)\n",
      "tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.7991, grad_fn=<SumBackward0>)\n",
      "tensor(0.0210, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9357, grad_fn=<SumBackward0>)\n",
      "tensor(3.6240e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.7370, grad_fn=<SumBackward0>)\n",
      "tensor(0.0259, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4691, grad_fn=<SumBackward0>)\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4016, grad_fn=<SumBackward0>)\n",
      "tensor(0.0205, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5004, grad_fn=<SumBackward0>)\n",
      "tensor(3.4777e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1547, grad_fn=<SumBackward0>)\n",
      "tensor(0.0194, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5391, grad_fn=<SumBackward0>)\n",
      "tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.6156, grad_fn=<SumBackward0>)\n",
      "tensor(0.0236, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6728, grad_fn=<SumBackward0>)\n",
      "tensor(7.0624e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1804, grad_fn=<SumBackward0>)\n",
      "tensor(0.0167, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2784, grad_fn=<SumBackward0>)\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3268, grad_fn=<SumBackward0>)\n",
      "tensor(0.0178, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7499, grad_fn=<SumBackward0>)\n",
      "tensor(5.1386e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4666, grad_fn=<SumBackward0>)\n",
      "tensor(0.0183, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1594, grad_fn=<SumBackward0>)\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1644, grad_fn=<SumBackward0>)\n",
      "tensor(0.0176, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6796, grad_fn=<SumBackward0>)\n",
      "tensor(7.8938e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.0986, grad_fn=<SumBackward0>)\n",
      "tensor(0.0150, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4606, grad_fn=<SumBackward0>)\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4157, grad_fn=<SumBackward0>)\n",
      "tensor(0.0165, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6279, grad_fn=<SumBackward0>)\n",
      "tensor(8.8172e-06, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.9741, grad_fn=<SumBackward0>)\n",
      "tensor(0.0120, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3221, grad_fn=<SumBackward0>)\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2904, grad_fn=<SumBackward0>)\n",
      "tensor(0.0163, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8550, grad_fn=<SumBackward0>)\n",
      "tensor(8.1090e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.3853, grad_fn=<SumBackward0>)\n",
      "tensor(0.0149, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2457, grad_fn=<SumBackward0>)\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2069, grad_fn=<SumBackward0>)\n",
      "tensor(0.0112, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.2673, grad_fn=<SumBackward0>)\n",
      "tensor(2.2350e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.6753, grad_fn=<SumBackward0>)\n",
      "tensor(0.0059, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3039, grad_fn=<SumBackward0>)\n",
      "tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3040, grad_fn=<SumBackward0>)\n",
      "tensor(0.0123, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7758, grad_fn=<SumBackward0>)\n",
      "tensor(9.0489e-06, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4204, grad_fn=<SumBackward0>)\n",
      "tensor(0.0106, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3592, grad_fn=<SumBackward0>)\n",
      "tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3508, grad_fn=<SumBackward0>)\n",
      "tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4997, grad_fn=<SumBackward0>)\n",
      "tensor(5.6810e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.0550, grad_fn=<SumBackward0>)\n",
      "tensor(0.0075, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4037, grad_fn=<SumBackward0>)\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4766, grad_fn=<SumBackward0>)\n",
      "tensor(0.0101, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5548, grad_fn=<SumBackward0>)\n",
      "tensor(3.2065e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.0213, grad_fn=<SumBackward0>)\n",
      "tensor(0.0064, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1333, grad_fn=<SumBackward0>)\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1312, grad_fn=<SumBackward0>)\n",
      "tensor(0.0079, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4014, grad_fn=<SumBackward0>)\n",
      "tensor(7.7452e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7707, grad_fn=<SumBackward0>)\n",
      "tensor(0.0053, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(0.9866, grad_fn=<SumBackward0>)\n",
      "tensor(3.4144e-05, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.9412, grad_fn=<SumBackward0>)\n",
      "tensor(0.0068, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "torch.Size([1000, 256])\n",
      "tensor(1.1247, grad_fn=<SumBackward0>)\n",
      "tensor(0.5529, grad_fn=<SumBackward0>)\n",
      "Architectures:  4\n",
      "Test-Accuracy:  75.28799999633789\n",
      "Zerocost proxy score:  78.9804458618164\n",
      "Computation Proxy Time:  3.5885672569274902\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9752\\3401350517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mproxy\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'synflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_synflow_per_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_arch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# align the input of the two proxies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m## store result zerocost score for compute correlation with accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9752\\3437627606.py\u001b[0m in \u001b[0;36mcompute_synflow_per_weight\u001b[1;34m(net, inputs, targets, loss_fn)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\supernet_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\supernet_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# start_time = time.time()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;31m# print(time.time()-start_time)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\supernet_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_layer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_layer_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_attn_dropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\module\\multihead_super.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;31m# the relative position embedding of V (N, N, head_dim) get shape like (N, B*num_heads, head_dim). We reshape it to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;31m# same size as x (B, num_heads, N, hidden_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mattn_1\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mr_p_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_num_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_scale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### get data sample to compute the proxies\n",
    "x, y = next(iter(data_loader_val))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "### initial loss function to cumpute gradients\n",
    "lossfunc = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "proxy = 'synflow' ##change proxy\n",
    "\n",
    "#get net and then compute  zero-cost-proxy\n",
    "proxy_scores = []\n",
    "accs = []\n",
    "st_time = time.time()\n",
    "# fifty architectures\n",
    "for arch,accuracy in arch_candidate_set.items():\n",
    "    if int(arch) == 50:\n",
    "        break\n",
    "    e_time = time.time()\n",
    "    ### initital net from API\n",
    "    net_setting = accuracy['net_setting']\n",
    "    net_arch = unwrap_model(model_VIT)\n",
    "    net_arch.set_sample_config(config=net_setting)\n",
    "    net_arch.to(device)\n",
    "\n",
    "    ##compute proxy\n",
    "    if proxy == 'grad_norm':\n",
    "        res = get_grad_norm_scores(net_arch, x, y,lossfunc)\n",
    "\n",
    "    elif proxy =='synflow':\n",
    "        res = compute_synflow_per_weight(net_arch,x, y, lossfunc)  # align the input of the two proxies\n",
    "\n",
    "    ## store result zerocost score for compute correlation with accuracy\n",
    "    del net_arch\n",
    "    print('Architectures: ',arch)\n",
    "    print('Test-Accuracy: ', accuracy['test-accuracy'])\n",
    "    proxy_scores.append(res)\n",
    "    accs.append(accuracy['test-accuracy'])\n",
    "    print('Zerocost proxy score: ',res)\n",
    "    edl_time = time.time()\n",
    "    print('Computation Proxy Time: ',edl_time-e_time)   \n",
    "    print('---------------------------------------------')\n",
    "end_time = time.time()\n",
    "print('total time: ',end_time-st_time)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8fafa",
   "metadata": {},
   "source": [
    "### 6. Evaluating Proxy Effectiveness\n",
    "This section focuses on evaluating the effectiveness of the proxy in predicting actual performance.\n",
    "\n",
    "**Correlation Analysis**: We will calculate Kendall correlation between the proxy scores and the test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8604dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###compute correlation with accuracy\n",
    "kendalltau = stats.kendalltau(proxy_scores, accs)\n",
    "print('*'*50)\n",
    "print('Kendalltau:', kendalltau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d18f2",
   "metadata": {},
   "source": [
    "### 7. Visualizing the Correlation Distribution\n",
    "This section visualizes the correlation between the proxy scores and the test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ded2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=proxy_scores, y=accs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Proxy Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920c397",
   "metadata": {},
   "source": [
    "### 8. Identifying Top Architectures based on Proxies\n",
    "This section focuses on identifying the top-performing architectures based on the computed proxy scores.\n",
    "\n",
    "**Ranking by Proxy Scores*: We will sort the ViT architectures in descending order based on their proxy scores. This ranking prioritizes architectures with higher predicted performance.\n",
    "\n",
    "**Top Architectures**: We can then identify the top-ranked architectures (e.g., top 1 or top 5) as potential candidates for further exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df163d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.get_subnet_Vision_Transformer_Arch import get_subnet_arch\n",
    "best_index_architectures = np.argmax(proxy_scores) #get the index of top-1 architecture based on the proxy score\n",
    "\n",
    "# load the top-1 architecture and check the layer-wise details\n",
    "net_arch = arch_candidate_set[str(best_index_architectures)]['net_setting']\n",
    "model_best_by_proxys = get_subnet_arch(net_arch)\n",
    "\n",
    "\n",
    "print(model_best_by_proxys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2cf1f",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[1] Chen, Minghao, Houwen Peng, Jianlong Fu, and Haibin Ling. \"Autoformer: Searching transformers for visual recognition.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12270-12280. 2021.\n",
    "\n",
    "[2] Li, Guihong, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, and Radu Marculescu. \"Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities.\" arXiv preprint arXiv:2307.01998 (2023).\n",
    "\n",
    "[3] Lee, Namhoon, Thalaiyasingam Ajanthan, and Philip HS Torr. \"Snip: Single-shot network pruning based on connection sensitivity.\" arXiv preprint arXiv:1810.02340 (2018)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoformerassign)",
   "language": "python",
   "name": "autoformerassign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
