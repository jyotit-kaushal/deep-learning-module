{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reverse-cathedral",
   "metadata": {},
   "source": [
    "# # HW - A Guided Tutorial on Proxies Estimating Performance of Vision Transformers\n",
    "**Author:** \n",
    "**Version:** \n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.7.16)\n",
    "- numpy==1.21.5\n",
    "- torch==1.13.1\n",
    "- torchvision==0.14.1\n",
    "- timm==0.4.12\n",
    "- opencv-python==4.9.0.80\n",
    "- scipy==1.7.3\n",
    "- scikit-image==0.19.2\n",
    "- pyyaml==5.4.1\n",
    "- easydict==1.13\n",
    "- matplotlib\n",
    "- ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-failure",
   "metadata": {},
   "source": [
    "### 0. Prelim: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boxed-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from timm.utils.model import unwrap_model\n",
    "from lib.datasets import build_dataset\n",
    "from lib import utils\n",
    "import json\n",
    "from scipy import stats\n",
    "import json\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6ce4",
   "metadata": {},
   "source": [
    "### 1. Prelim: Hyperparameters and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277741d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "api_Auto_FM_Benchmark='./AutoFM_CVPR2022_API_5_7M.json'\n",
    "input_size=224\n",
    "data_path='./dataset/imagenet'              \n",
    "seed=0\n",
    "num_workers=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b36d1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "# The code is cpu-friendly, but running the notebook with GPU(s) will drastically speed up the computation.\n",
    "\n",
    "# Define device for torch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed9097",
   "metadata": {},
   "source": [
    "### 2. Prelim: Dataset and Dataloader\n",
    "\n",
    "This section loads data into PyTorch Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba04c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "# Fixed the seed for reproducibility\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True  # enables automatic selection of the most efficient algorithms for deep learning operations\n",
    "\n",
    "##load data\n",
    "dataset_val, nb_classes = build_dataset(True, data_path,input_size, \"train\")  # load data of one batch\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "    \n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size,\n",
    "    sampler=sampler_val, num_workers=num_workers,\n",
    "    pin_memory=True, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d68cf",
   "metadata": {},
   "source": [
    "### 3. Use AutoFormer API to introduce candidate architectures\n",
    "This section loads the candidate architectures via an external API [1] and use it to explore Vision Transformers from an architectural perspective. 50 architectures are introduced. For each architecture, the API will provide:\n",
    "\n",
    "1) the architectural configuration;\n",
    "\n",
    "2) the test accuracy (for image classification) on ImageNet-1k validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adaf02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "### load candidate architectures\n",
    "from model.get_Vision_Transformer_Arch import model_VIT\n",
    "file_api = open(api_Auto_FM_Benchmark)\n",
    "arch_candidate_set =json.load(file_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b9fa4",
   "metadata": {},
   "source": [
    "### 4. Develop functions to compute Proxies for Architecture Performance Estimation\n",
    "\n",
    "This section focuses on the two proxies that estimate the performance of candidate Vision Transformer architectures\n",
    "\n",
    "**Question 2**: Write your proxy\n",
    "\n",
    "The goal is to design a proxy (function) that can predict the performance of some given networks without training the network parameters [2]. We will consider two simple proxies:\n",
    "\n",
    "**SNIP**: This proxy measures the each weight multiplied by its gradient [2,3] at initialization.\n",
    "\n",
    "**Gradient Norm (grad_norm)**: This proxy measures the norm of the gradients by layer [2] at initialization. A lower gradient norm may signify training difficulties, though a high norm could indicate the problem of exploding gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b11b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_metric_array_grad_norm(net, metric):\n",
    "    metric_array = []\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return sum(metric_array).item()\n",
    "\n",
    "def get_grad_norm_scores(net, inputs, targets, loss_fn):\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    net.zero_grad()\n",
    "    outputs = net.forward(inputs)\n",
    "    \n",
    "    loss = loss_fn(outputs, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norm_arr = get_layer_metric_array_grad_norm(net, lambda l: l.weight.grad.norm() if l.weight.grad is not None else torch.zeros_like(l.weight))\n",
    "    \n",
    "    return grad_norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5d18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_metric_array_synflow(net, metric):\n",
    "    metric_array = []\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return metric_array\n",
    "\n",
    "def compute_synflow_per_weight(net, inputs, targets, loss_fn):\n",
    "\n",
    "    device = inputs.device\n",
    "\n",
    "    #convert params to their abs. Keep sign for converting it back.\n",
    "    @torch.no_grad()\n",
    "    def linearize(net):\n",
    "        signs = {}\n",
    "        for name, param in net.state_dict().items():\n",
    "            signs[name] = torch.sign(param)\n",
    "            param.abs_()\n",
    "        return signs\n",
    "\n",
    "    #convert to orig values\n",
    "    @torch.no_grad()\n",
    "    def nonlinearize(net, signs):\n",
    "        for name, param in net.state_dict().items():\n",
    "            if 'weight_mask' not in name:\n",
    "                param.mul_(signs[name])\n",
    "\n",
    "    # keep signs of all params\n",
    "    signs = linearize(net)\n",
    "    \n",
    "    # Compute gradients with input of 1s \n",
    "    net.zero_grad()\n",
    "    net.float()\n",
    "    outputs = net.forward(inputs)\n",
    "    \n",
    "    loss = loss_fn(outputs, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # select the gradients that we want to use for search/prune\n",
    "    def synflow(layer):\n",
    "        if layer.weight.grad is not None:\n",
    "            return torch.abs(layer.weight * layer.weight.grad)\n",
    "        else:\n",
    "            return torch.zeros_like(layer.weight)\n",
    "    ## computed synflow score for each layer\n",
    "    grads_abs = get_layer_metric_array_synflow(net, synflow)\n",
    "\n",
    "    ## sum synflow score for all layer\n",
    "    def sum_arr(arr):\n",
    "        sum = 0.\n",
    "        for i in range(len(arr)):\n",
    "            print(arr[i].shape)\n",
    "            print(arr[i].sum())\n",
    "            print(arr[i][0].sum())\n",
    "            sum += torch.sum(arr[i])\n",
    "        return sum.item()\n",
    "\n",
    "    grads_abs = sum_arr(grads_abs)\n",
    "    # apply signs of all params\n",
    "    nonlinearize(net, signs)\n",
    "\n",
    "    return grads_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c07b6",
   "metadata": {},
   "source": [
    "### 5. Computing Proxies for Estimating Performance of Candidate Architectures\n",
    "This section computes the proxy scores for the candidate Vision Transformer architectures from the API.\n",
    "\n",
    "**Leveraging Vision Transformer architectures candidates information**: We will utilize the information retrieved from the API for each Vision Transformer architecture:\n",
    "\n",
    "- **Architecture configuration**: This allows us to create the corresponding PyTorch model representing the specific Vision Transformer architectures candidates.\n",
    "- **Test accuracy**: While the API might provide test accuracy, we won't rely on it directly for ranking here.\n",
    "\n",
    "**Proxy Computation**: We will employ the proxy function: **grad_norm** and **SynFlow** (defined earlier) to compute a score for each Vision Transformer architectures candidate. This score estimates the potential performance of the corresponding Vision Transformer architecture without actually training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23c0415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 16, 16])\n",
      "tensor(19.1641, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7682, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4427, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.7797, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.7506, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9732, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.7208, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2248, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2349, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6773, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4500, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3707, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4180, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6771, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1591, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3248, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3280, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8314, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5408, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4612, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4780, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9032, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4582, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2876, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2348, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4534, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.8373, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5200, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4962, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8874, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.3816, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4645, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3668, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4948, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.9433, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1753, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1331, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.3797, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7258, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3333, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3105, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7535, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5218, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.2398, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.2796, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.3965, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7290, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(0.9025, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.8848, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8084, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4475, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1180, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.0812, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7692, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4597, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1226, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1085, grad_fn=<SumBackward0>)\n",
      "torch.Size([1000, 256])\n",
      "tensor(1.1744, grad_fn=<SumBackward0>)\n",
      "Architectures:  0\n",
      "Test-Accuracy:  75.33999999755859\n",
      "Zerocost proxy score:  84.35889434814453\n",
      "Computation Proxy Time:  3.4033803939819336\n",
      "---------------------------------------------\n",
      "torch.Size([256, 3, 16, 16])\n",
      "tensor(20.7310, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8284, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4999, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.8591, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.8497, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7757, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4065, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5085, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4442, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7064, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.5170, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4378, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4537, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7074, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2367, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3982, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4010, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6514, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2330, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5709, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.5855, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7250, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.2126, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3633, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3602, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.6834, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1653, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(2.0019, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.8848, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.9464, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4731, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5616, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4633, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5945, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1074, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(0.9658, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.9511, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.8105, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.4876, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.4147, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.4232, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4043, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7789, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.5025, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.5549, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.7066, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1747, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.1974, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.1764, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.5948, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(1.1018, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.0015, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(0.9767, grad_fn=<SumBackward0>)\n",
      "torch.Size([768, 256])\n",
      "tensor(0.4418, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 256])\n",
      "tensor(0.7935, grad_fn=<SumBackward0>)\n",
      "torch.Size([1024, 256])\n",
      "tensor(1.3573, grad_fn=<SumBackward0>)\n",
      "torch.Size([256, 1024])\n",
      "tensor(1.3242, grad_fn=<SumBackward0>)\n",
      "torch.Size([1000, 256])\n",
      "tensor(1.1882, grad_fn=<SumBackward0>)\n",
      "Architectures:  1\n",
      "Test-Accuracy:  75.21799999877929\n",
      "Zerocost proxy score:  88.67345428466797\n",
      "Computation Proxy Time:  3.3923401832580566\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24320\\3401350517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mproxy\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'synflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_synflow_per_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_arch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# align the input of the two proxies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m## store result zerocost score for compute correlation with accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24320\\2121514673.py\u001b[0m in \u001b[0;36mcompute_synflow_per_weight\u001b[1;34m(net, inputs, targets, loss_fn)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\supernet_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\supernet_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# start_time = time.time()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;31m# print(time.time()-start_time)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\supernet_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_layer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_layer_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_attn_dropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\module\\multihead_super.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mqkv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_num_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m# make torchscript happy (cannot use tensor as tuple)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Zerocost_AutoFormer_revised_right\\model\\module\\qkv_super.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bias'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalc_sampled_param_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### get data sample to compute the proxies\n",
    "x, y = next(iter(data_loader_val))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "### initial loss function to cumpute gradients\n",
    "lossfunc = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "proxy = 'synflow' ##change proxy\n",
    "\n",
    "#get net and then compute  zero-cost-proxy\n",
    "proxy_scores = []\n",
    "accs = []\n",
    "st_time = time.time()\n",
    "# fifty architectures\n",
    "for arch,accuracy in arch_candidate_set.items():\n",
    "    if int(arch) == 50:\n",
    "        break\n",
    "    e_time = time.time()\n",
    "    ### initital net from API\n",
    "    net_setting = accuracy['net_setting']\n",
    "    net_arch = unwrap_model(model_VIT)\n",
    "    net_arch.set_sample_config(config=net_setting)\n",
    "    net_arch.to(device)\n",
    "\n",
    "    ##compute proxy\n",
    "    if proxy == 'grad_norm':\n",
    "        res = get_grad_norm_scores(net_arch, x, y,lossfunc)\n",
    "\n",
    "    elif proxy =='synflow':\n",
    "        res = compute_synflow_per_weight(net_arch,x, y, lossfunc)  # align the input of the two proxies\n",
    "\n",
    "    ## store result zerocost score for compute correlation with accuracy\n",
    "    del net_arch\n",
    "    print('Architectures: ',arch)\n",
    "    print('Test-Accuracy: ', accuracy['test-accuracy'])\n",
    "    proxy_scores.append(res)\n",
    "    accs.append(accuracy['test-accuracy'])\n",
    "    print('Zerocost proxy score: ',res)\n",
    "    edl_time = time.time()\n",
    "    print('Computation Proxy Time: ',edl_time-e_time)   \n",
    "    print('---------------------------------------------')\n",
    "end_time = time.time()\n",
    "print('total time: ',end_time-st_time)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8fafa",
   "metadata": {},
   "source": [
    "### 6. Evaluating Proxy Effectiveness\n",
    "This section focuses on evaluating the effectiveness of the proxy in predicting actual performance.\n",
    "\n",
    "**Correlation Analysis**: We will calculate Kendall correlation between the proxy scores and the test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8604dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###compute correlation with accuracy\n",
    "kendalltau = stats.kendalltau(proxy_scores, accs)\n",
    "print('*'*50)\n",
    "print('Kendalltau:', kendalltau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d18f2",
   "metadata": {},
   "source": [
    "### 7. Visualizing the Correlation Distribution\n",
    "This section visualizes the correlation between the proxy scores and the test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ded2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=proxy_scores, y=accs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Proxy Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920c397",
   "metadata": {},
   "source": [
    "### 8. Identifying Top Architectures based on Proxies\n",
    "This section focuses on identifying the top-performing architectures based on the computed proxy scores.\n",
    "\n",
    "**Ranking by Proxy Scores*: We will sort the ViT architectures in descending order based on their proxy scores. This ranking prioritizes architectures with higher predicted performance.\n",
    "\n",
    "**Top Architectures**: We can then identify the top-ranked architectures (e.g., top 1 or top 5) as potential candidates for further exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df163d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.get_subnet_Vision_Transformer_Arch import get_subnet_arch\n",
    "best_index_architectures = np.argmax(proxy_scores) #get the index of top-1 architecture based on the proxy score\n",
    "\n",
    "# load the top-1 architecture and check the layer-wise details\n",
    "net_arch = arch_candidate_set[str(best_index_architectures)]['net_setting']\n",
    "model_best_by_proxys = get_subnet_arch(net_arch)\n",
    "\n",
    "\n",
    "print(model_best_by_proxys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2cf1f",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[1] Chen, Minghao, Houwen Peng, Jianlong Fu, and Haibin Ling. \"Autoformer: Searching transformers for visual recognition.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12270-12280. 2021.\n",
    "\n",
    "[2] Li, Guihong, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, and Radu Marculescu. \"Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities.\" arXiv preprint arXiv:2307.01998 (2023).\n",
    "\n",
    "[3] Lee, Namhoon, Thalaiyasingam Ajanthan, and Philip HS Torr. \"Snip: Single-shot network pruning based on connection sensitivity.\" arXiv preprint arXiv:1810.02340 (2018)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoformerassign)",
   "language": "python",
   "name": "autoformerassign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
