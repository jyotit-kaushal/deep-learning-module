{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa828e92",
   "metadata": {},
   "source": [
    "### - This notebook keeps the code (version: Mar 27, 12:54am, i.e., the last version coded by Tuan), but it is commented by Xinda to detail why some lines are removed/revised.\n",
    "\n",
    "### - One may search '# [Xinda]' for these comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ecb4c",
   "metadata": {},
   "source": [
    "# # HW - A Guided Tutorial on Proxies Estimating Performance of Vision Transformers\n",
    "**Author:** \n",
    "**Version:** \n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.7.16)\n",
    "- numpy==1.21.5\n",
    "- torch==1.13.1\n",
    "- torchvision==0.14.1\n",
    "- timm==0.4.12\n",
    "- opencv-python==4.9.0.80\n",
    "- scipy==1.7.3\n",
    "- scikit-image==0.19.2\n",
    "- pyyaml==5.4.1\n",
    "- easydict==1.13\n",
    "- matplotlib # [Xinda] this package is required\n",
    "- ipykernel  # [Xinda] this package is required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-failure",
   "metadata": {},
   "source": [
    "### 0. Prelim: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boxed-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from timm.utils.model import unwrap_model\n",
    "from lib.datasets import build_dataset\n",
    "from lib import utils\n",
    "import json\n",
    "from scipy import stats\n",
    "import json\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6ce4",
   "metadata": {},
   "source": [
    "### 1. Prelim: Hyperparameters and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277741d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "api_Auto_FM_Benchmark='./AutoFM_CVPR2022_API_5_7M.json'\n",
    "input_size=224\n",
    "data_path='./dataset/imagenet'              \n",
    "seed=0\n",
    "num_workers=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b36d1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "# The code is cpu-friendly, but running the notebook with GPU(s) will drastically speed up the computation.\n",
    "\n",
    "# Define device for torch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed9097",
   "metadata": {},
   "source": [
    "### 2. Prelim: Dataset and Dataloader\n",
    "\n",
    "This section loads data into PyTorch Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba04c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\autoformerassign\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "# Fixed the seed for reproducibility\n",
    "\n",
    "seed = seed # [Xinda] unnecessary, removed; [Xinda] Q: why 'utils.get_rank'? Remove it if unnecessary, otherwise please comment to explain [Tuan]: Yes. I have remove it\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True  # enables automatic selection of the most efficient algorithms for deep learning operations\n",
    "\n",
    "##load data\n",
    "dataset_val, nb_classes = build_dataset(True, data_path,input_size, \"train\")  # load data of one batch\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "    \n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size,\n",
    "    sampler=sampler_val, num_workers=num_workers,\n",
    "    pin_memory=True, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d68cf",
   "metadata": {},
   "source": [
    "### 3. Use AutoFormer API to introduce candidate architectures\n",
    "This section loads the candidate architectures via an external API [1] and use it to explore Vision Transformers from an architectural perspective. 50 architectures are introduced. For each architecture, the API will provide:\n",
    "\n",
    "1) the architectural configuration;\n",
    "\n",
    "2) the test accuracy (for image classification) on ImageNet-1k validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adaf02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "### load candidate architectures\n",
    "from model.get_Vision_Transformer_Arch import model_VIT\n",
    "file_api = open(api_Auto_FM_Benchmark)\n",
    "arch_candidate_set =json.load(file_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b9fa4",
   "metadata": {},
   "source": [
    "### 4. Develop functions to compute Proxies for Architecture Performance Estimation\n",
    "\n",
    "This section focuses on the two proxies that estimate the performance of candidate Vision Transformer architectures\n",
    "\n",
    "**Question 2**: Write your proxy\n",
    "\n",
    "The goal is to design a proxy (function) that can predict the performance of some given networks without training the network parameters [2]. We will consider two simple proxies:\n",
    "\n",
    "**SynFlow**: This proxy measures the absolute value of each weight multiplied by its gradient [3] at initialization.\n",
    "\n",
    "**Gradient Norm (grad_norm)**: This proxy measures the norm of the gradients by layer [2] at initialization. A lower gradient norm may signify training difficulties, though a high norm could indicate the problem of exploding gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b11b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_metric_array_grad_norm(net, metric, mode):  # [Xinda] Q: Is the parameter 'mode' from AutoFormer? It seems it is for pruning. If it is not a must-use parameter, delete it.\n",
    "    metric_array = []\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if mode=='channel' and hasattr(layer,'dont_ch_prune'):  # [Xinda] Q: Are these two lines necessary? Delete them if not.\n",
    "            continue  # [Xinda] these are for pruning applications, removed.\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return sum(metric_array).item()\n",
    "\n",
    "def get_grad_norm_scores(net, inputs, targets, loss_fn):\n",
    "    for param in net.parameters():  # [Xinda] 'param.requires_grad' is by default 'True', removed\n",
    "        param.requires_grad = True\n",
    "\n",
    "    net.zero_grad() \n",
    "\n",
    "    outputs = net.forward(inputs)\n",
    "    \n",
    "    loss = loss_fn(outputs, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_norm_arr = get_layer_metric_array_grad_norm(net, lambda l: l.weight.grad.norm() if l.weight.grad is not None else torch.zeros_like(l.weight), mode='param')  # [Xinda] revised for better alignment among the proxy functions\n",
    "    \n",
    "    return grad_norm_arr\n",
    "\n",
    "# [Xinda] The function below is almost identical to 'get_layer_metric_array_grad_norm', redundant, removed.\n",
    "def get_layer_metric_array_synflow(net, metric, mode):  # [Xinda] Q: Is 'mode' necessary? Delete it if it is not.\n",
    "    metric_array = []\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if mode=='channel' and hasattr(layer,'dont_ch_prune'):  # [Xinda] Q: Are these two lines necessary? Delete them if not.\n",
    "            continue\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return metric_array\n",
    "\n",
    "# [Xinda] the functions 'linearize' and 'nonlinearize' are from the code of MeCo repo (for synflow), but their code is wrong. synflow is rewritten\n",
    "def compute_synflow_per_weight(net, inputs,target,loss_fn):\n",
    "\n",
    "    device = inputs.device  # [Xinda] not in use, removed\n",
    "\n",
    "    #convert params to their abs. Keep sign for converting it back.\n",
    "    @torch.no_grad()\n",
    "    def linearize(net):\n",
    "        signs = {}\n",
    "        for name, param in net.state_dict().items():\n",
    "            signs[name] = torch.sign(param)\n",
    "            param.abs_()\n",
    "        return signs\n",
    "\n",
    "    #convert to orig values\n",
    "    @torch.no_grad()\n",
    "    def nonlinearize(net, signs):\n",
    "        for name, param in net.state_dict().items():\n",
    "            if 'weight_mask' not in name:\n",
    "                param.mul_(signs[name])\n",
    "\n",
    "    # keep signs of all params\n",
    "    signs = linearize(net)\n",
    "    \n",
    "    # Compute gradients with input of 1s \n",
    "    net.zero_grad()\n",
    "    net.float()\n",
    "    input_dim = list(inputs[0,:].shape)  # [Xinda] revised to align the inputs for all proxies; [Xinda] Q: Are these two lines necessary? Delete them if not. [Tuan]: Indeed. we need to get input_dim\n",
    "    inputs = torch.ones([1] + input_dim).float().to(device)  \n",
    "    output = net.forward(inputs)\n",
    "    torch.sum(output).backward()  # [Xinda] revised to align the loss function for all proxies; [Xinda] Q: It seems there is a discrepancy between the loss functions of grad_norm and synflow. It would be better to align them, e.g., also use CrossEntropyLoss for synflow. [Tuan] Thanks XinDa. Yes. This is original code of synflow, and the input image have convert to all pixels equal 1 before feed to network\n",
    "\n",
    "    # select the gradients that we want to use for search/prune\n",
    "    def synflow(layer):\n",
    "        if layer.weight.grad is not None:\n",
    "            return torch.abs(layer.weight * layer.weight.grad)\n",
    "        else:  # [Xinda] unnecessary for zero-shot NAS application, removed\n",
    "            return torch.zeros_like(layer.weight)\n",
    "    ## computed synflow score for each layer\n",
    "    grads_abs = get_layer_metric_array_synflow(net, synflow, mode='param')\n",
    "\n",
    "    ## sum synflow score for all layer  # [Xinda] can be simplified, removed\n",
    "    def sum_arr(arr):\n",
    "        sum = 0.\n",
    "        for i in range(len(arr)):\n",
    "            sum += torch.sum(arr[i])\n",
    "        return sum.item()\n",
    "\n",
    "    grads_abs = sum_arr(grads_abs)\n",
    "    # apply signs of all params\n",
    "    nonlinearize(net, signs)\n",
    "\n",
    "    return grads_abs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c07b6",
   "metadata": {},
   "source": [
    "### 5. Computing Proxies for Estimating Performance of Candidate Architectures\n",
    "This section computes the proxy scores for the candidate Vision Transformer architectures from the API.\n",
    "\n",
    "**Leveraging Vision Transformer architectures candidates information**: We will utilize the information retrieved from the API for each Vision Transformer architecture:\n",
    "\n",
    "- **Architecture configuration**: This allows us to create the corresponding PyTorch model representing the specific Vision Transformer architectures candidates.  # [Xinda] Q: Is it true? Better revise it if it is not. [tuan] It's True, XinDa. I have aready implement code to get subnet to model in pytorch\n",
    "- **Test accuracy**: While the API might provide test accuracy, we won't rely on it directly for ranking here.\n",
    "\n",
    "**Proxy Computation**: We will employ the proxy function: **grad_norm** and **SynFlow** (defined earlier) to compute a score for each Vision Transformer architectures candidate. This score estimates the potential performance of the corresponding Vision Transformer architecture without actually training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23c0415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architectures:  0\n",
      "Test-Accuracy:  75.33999999755859\n",
      "Zerocost proxy score:  30.41531753540039\n",
      "Computation Proxy Time:  3.6419730186462402\n",
      "---------------------------------------------\n",
      "Architectures:  1\n",
      "Test-Accuracy:  75.21799999877929\n",
      "Zerocost proxy score:  30.544662475585938\n",
      "Computation Proxy Time:  3.283215284347534\n",
      "---------------------------------------------\n",
      "Architectures:  2\n",
      "Test-Accuracy:  75.29799999755859\n",
      "Zerocost proxy score:  31.0082950592041\n",
      "Computation Proxy Time:  3.140200614929199\n",
      "---------------------------------------------\n",
      "Architectures:  3\n",
      "Test-Accuracy:  75.3259999987793\n",
      "Zerocost proxy score:  30.170011520385742\n",
      "Computation Proxy Time:  3.3083443641662598\n",
      "---------------------------------------------\n",
      "Architectures:  4\n",
      "Test-Accuracy:  75.28799999633789\n",
      "Zerocost proxy score:  27.07955551147461\n",
      "Computation Proxy Time:  3.039320468902588\n",
      "---------------------------------------------\n",
      "Architectures:  5\n",
      "Test-Accuracy:  75.2159999975586\n",
      "Zerocost proxy score:  33.13892364501953\n",
      "Computation Proxy Time:  3.6690728664398193\n",
      "---------------------------------------------\n",
      "Architectures:  6\n",
      "Test-Accuracy:  75.2120000024414\n",
      "Zerocost proxy score:  30.5817813873291\n",
      "Computation Proxy Time:  3.335803985595703\n",
      "---------------------------------------------\n",
      "Architectures:  7\n",
      "Test-Accuracy:  75.28799999511719\n",
      "Zerocost proxy score:  31.823577880859375\n",
      "Computation Proxy Time:  3.4085988998413086\n",
      "---------------------------------------------\n",
      "Architectures:  8\n",
      "Test-Accuracy:  75.2899999975586\n",
      "Zerocost proxy score:  31.102304458618164\n",
      "Computation Proxy Time:  4.0200724601745605\n",
      "---------------------------------------------\n",
      "Architectures:  9\n",
      "Test-Accuracy:  75.236\n",
      "Zerocost proxy score:  29.280075073242188\n",
      "Computation Proxy Time:  3.983283519744873\n",
      "---------------------------------------------\n",
      "Architectures:  10\n",
      "Test-Accuracy:  74.86399999511718\n",
      "Zerocost proxy score:  26.548654556274414\n",
      "Computation Proxy Time:  3.204977035522461\n",
      "---------------------------------------------\n",
      "Architectures:  11\n",
      "Test-Accuracy:  75.218\n",
      "Zerocost proxy score:  30.042713165283203\n",
      "Computation Proxy Time:  3.4003400802612305\n",
      "---------------------------------------------\n",
      "Architectures:  12\n",
      "Test-Accuracy:  74.79199999633789\n",
      "Zerocost proxy score:  27.73396873474121\n",
      "Computation Proxy Time:  3.3010287284851074\n",
      "---------------------------------------------\n",
      "Architectures:  13\n",
      "Test-Accuracy:  75.1999999963379\n",
      "Zerocost proxy score:  27.800430297851562\n",
      "Computation Proxy Time:  3.590552806854248\n",
      "---------------------------------------------\n",
      "Architectures:  14\n",
      "Test-Accuracy:  74.58199999633788\n",
      "Zerocost proxy score:  27.046016693115234\n",
      "Computation Proxy Time:  3.221599817276001\n",
      "---------------------------------------------\n",
      "Architectures:  15\n",
      "Test-Accuracy:  75.422\n",
      "Zerocost proxy score:  30.33768081665039\n",
      "Computation Proxy Time:  3.9000585079193115\n",
      "---------------------------------------------\n",
      "Architectures:  16\n",
      "Test-Accuracy:  74.75399999267579\n",
      "Zerocost proxy score:  26.229270935058594\n",
      "Computation Proxy Time:  3.505958080291748\n",
      "---------------------------------------------\n",
      "Architectures:  17\n",
      "Test-Accuracy:  75.18199999511718\n",
      "Zerocost proxy score:  28.090816497802734\n",
      "Computation Proxy Time:  3.5873961448669434\n",
      "---------------------------------------------\n",
      "Architectures:  18\n",
      "Test-Accuracy:  74.81999999633788\n",
      "Zerocost proxy score:  27.54469871520996\n",
      "Computation Proxy Time:  3.5089974403381348\n",
      "---------------------------------------------\n",
      "Architectures:  19\n",
      "Test-Accuracy:  75.3079999987793\n",
      "Zerocost proxy score:  30.957157135009766\n",
      "Computation Proxy Time:  4.279336452484131\n",
      "---------------------------------------------\n",
      "Architectures:  20\n",
      "Test-Accuracy:  75.3299999987793\n",
      "Zerocost proxy score:  30.34317970275879\n",
      "Computation Proxy Time:  4.281912326812744\n",
      "---------------------------------------------\n",
      "Architectures:  21\n",
      "Test-Accuracy:  74.69999999511718\n",
      "Zerocost proxy score:  27.49073028564453\n",
      "Computation Proxy Time:  7.626359939575195\n",
      "---------------------------------------------\n",
      "Architectures:  22\n",
      "Test-Accuracy:  74.63199999389649\n",
      "Zerocost proxy score:  27.24589729309082\n",
      "Computation Proxy Time:  7.004863262176514\n",
      "---------------------------------------------\n",
      "Architectures:  23\n",
      "Test-Accuracy:  74.99399999023437\n",
      "Zerocost proxy score:  32.43037414550781\n",
      "Computation Proxy Time:  6.2124528884887695\n",
      "---------------------------------------------\n",
      "Architectures:  24\n",
      "Test-Accuracy:  75.20199999755859\n",
      "Zerocost proxy score:  29.880298614501953\n",
      "Computation Proxy Time:  5.859713315963745\n",
      "---------------------------------------------\n",
      "Architectures:  25\n",
      "Test-Accuracy:  75.3119999987793\n",
      "Zerocost proxy score:  28.52354621887207\n",
      "Computation Proxy Time:  5.165754318237305\n",
      "---------------------------------------------\n",
      "Architectures:  26\n",
      "Test-Accuracy:  74.74399999389648\n",
      "Zerocost proxy score:  27.73340606689453\n",
      "Computation Proxy Time:  4.801316738128662\n",
      "---------------------------------------------\n",
      "Architectures:  27\n",
      "Test-Accuracy:  75.1599999975586\n",
      "Zerocost proxy score:  29.21151351928711\n",
      "Computation Proxy Time:  5.195380449295044\n",
      "---------------------------------------------\n",
      "Architectures:  28\n",
      "Test-Accuracy:  75.142\n",
      "Zerocost proxy score:  30.705671310424805\n",
      "Computation Proxy Time:  5.677517414093018\n",
      "---------------------------------------------\n",
      "Architectures:  29\n",
      "Test-Accuracy:  75.40599999511718\n",
      "Zerocost proxy score:  30.60972785949707\n",
      "Computation Proxy Time:  5.621503591537476\n",
      "---------------------------------------------\n",
      "Architectures:  30\n",
      "Test-Accuracy:  75.29799999755859\n",
      "Zerocost proxy score:  29.636262893676758\n",
      "Computation Proxy Time:  5.621691942214966\n",
      "---------------------------------------------\n",
      "Architectures:  31\n",
      "Test-Accuracy:  75.3519999963379\n",
      "Zerocost proxy score:  30.276687622070312\n",
      "Computation Proxy Time:  5.553180456161499\n",
      "---------------------------------------------\n",
      "Architectures:  32\n",
      "Test-Accuracy:  75.4059999963379\n",
      "Zerocost proxy score:  30.40947723388672\n",
      "Computation Proxy Time:  5.569766283035278\n",
      "---------------------------------------------\n",
      "Architectures:  33\n",
      "Test-Accuracy:  75.324\n",
      "Zerocost proxy score:  30.717273712158203\n",
      "Computation Proxy Time:  5.672118663787842\n",
      "---------------------------------------------\n",
      "Architectures:  34\n",
      "Test-Accuracy:  74.70599999389648\n",
      "Zerocost proxy score:  26.546810150146484\n",
      "Computation Proxy Time:  4.805185317993164\n",
      "---------------------------------------------\n",
      "Architectures:  35\n",
      "Test-Accuracy:  75.23199999633789\n",
      "Zerocost proxy score:  27.71875762939453\n",
      "Computation Proxy Time:  5.170412540435791\n",
      "---------------------------------------------\n",
      "Architectures:  36\n",
      "Test-Accuracy:  74.9339999987793\n",
      "Zerocost proxy score:  26.42104721069336\n",
      "Computation Proxy Time:  4.939684629440308\n",
      "---------------------------------------------\n",
      "Architectures:  37\n",
      "Test-Accuracy:  75.31199999755859\n",
      "Zerocost proxy score:  29.546415328979492\n",
      "Computation Proxy Time:  5.555768728256226\n",
      "---------------------------------------------\n",
      "Architectures:  38\n",
      "Test-Accuracy:  75.12999999633789\n",
      "Zerocost proxy score:  29.80451202392578\n",
      "Computation Proxy Time:  6.3254523277282715\n",
      "---------------------------------------------\n",
      "Architectures:  39\n",
      "Test-Accuracy:  75.33599999755859\n",
      "Zerocost proxy score:  30.895584106445312\n",
      "Computation Proxy Time:  7.06645655632019\n",
      "---------------------------------------------\n",
      "Architectures:  40\n",
      "Test-Accuracy:  75.1859999975586\n",
      "Zerocost proxy score:  31.399669647216797\n",
      "Computation Proxy Time:  6.180301189422607\n",
      "---------------------------------------------\n",
      "Architectures:  41\n",
      "Test-Accuracy:  74.96999998535156\n",
      "Zerocost proxy score:  31.49580955505371\n",
      "Computation Proxy Time:  5.142409563064575\n",
      "---------------------------------------------\n",
      "Architectures:  42\n",
      "Test-Accuracy:  75.3759999963379\n",
      "Zerocost proxy score:  29.606719970703125\n",
      "Computation Proxy Time:  5.634489297866821\n",
      "---------------------------------------------\n",
      "Architectures:  43\n",
      "Test-Accuracy:  75.4019999987793\n",
      "Zerocost proxy score:  30.127599716186523\n",
      "Computation Proxy Time:  6.1135265827178955\n",
      "---------------------------------------------\n",
      "Architectures:  44\n",
      "Test-Accuracy:  75.1239999987793\n",
      "Zerocost proxy score:  29.503644943237305\n",
      "Computation Proxy Time:  5.2305989265441895\n",
      "---------------------------------------------\n",
      "Architectures:  45\n",
      "Test-Accuracy:  75.04799999755859\n",
      "Zerocost proxy score:  28.877283096313477\n",
      "Computation Proxy Time:  5.221593379974365\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architectures:  46\n",
      "Test-Accuracy:  75.2299999963379\n",
      "Zerocost proxy score:  28.344905853271484\n",
      "Computation Proxy Time:  5.1931445598602295\n",
      "---------------------------------------------\n",
      "Architectures:  47\n",
      "Test-Accuracy:  74.84799999023437\n",
      "Zerocost proxy score:  31.480377197265625\n",
      "Computation Proxy Time:  5.27339506149292\n",
      "---------------------------------------------\n",
      "Architectures:  48\n",
      "Test-Accuracy:  75.28199999633789\n",
      "Zerocost proxy score:  28.543542861938477\n",
      "Computation Proxy Time:  5.406125545501709\n",
      "---------------------------------------------\n",
      "Architectures:  49\n",
      "Test-Accuracy:  75.27200000366211\n",
      "Zerocost proxy score:  30.449434280395508\n",
      "Computation Proxy Time:  6.226221561431885\n",
      "---------------------------------------------\n",
      "total time:  239.68043112754822\n"
     ]
    }
   ],
   "source": [
    "### get data sample to compute the proxies\n",
    "x, y = next(iter(data_loader_val))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "### initial loss function to cumpute gradients\n",
    "lossfunc = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "proxy = 'grad_norm' ##change proxy\n",
    "\n",
    "#get net and then compute  zero-cost-proxy\n",
    "proxy_scores = []\n",
    "accs = []\n",
    "st_time = time.time()\n",
    "# fifty architectures\n",
    "for arch,accuracy in arch_candidate_set.items():\n",
    "    if int(arch) == 50:\n",
    "        break\n",
    "    e_time = time.time()\n",
    "    ### initital net from API\n",
    "    net_setting = accuracy['net_setting']\n",
    "    net_arch = unwrap_model(model_VIT)\n",
    "    net_arch.set_sample_config(config=net_setting)\n",
    "    net_arch.to(device)\n",
    "\n",
    "    ##compute proxy\n",
    "    if proxy == 'grad_norm':\n",
    "        res = get_grad_norm_scores(net_arch, x, y,lossfunc)\n",
    "\n",
    "    elif proxy =='synflow':\n",
    "        res = compute_synflow_per_weight(net_arch,x)\n",
    "\n",
    "    ## store result zerocost score for compute correlation with accuracy\n",
    "    del net_arch\n",
    "    print('Architectures: ',arch)\n",
    "    print('Test-Accuracy: ', accuracy['test-accuracy'])\n",
    "    proxy_scores.append(res)\n",
    "    accs.append(accuracy['test-accuracy'])\n",
    "    print('Zerocost proxy score: ',res)\n",
    "    edl_time = time.time()\n",
    "    print('Computation Proxy Time: ',edl_time-e_time)   \n",
    "    print('---------------------------------------------')\n",
    "end_time = time.time()\n",
    "print('total time: ',end_time-st_time)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8fafa",
   "metadata": {},
   "source": [
    "### 6. Evaluating Proxy Effectiveness\n",
    "This section focuses on evaluating the effectiveness of the proxy in predicting actual performance.\n",
    "\n",
    "**Correlation Analysis**: We will calculate Kendall correlation between the proxy scores and the test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8604dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Kendalltau: KendalltauResult(correlation=0.29563089438851525, pvalue=0.0024602786448541963)\n"
     ]
    }
   ],
   "source": [
    "###compute correlation with accuracy\n",
    "kendalltau = stats.kendalltau(proxy_scores, accs)\n",
    "print('*'*50)\n",
    "print('Kendalltau:', kendalltau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d18f2",
   "metadata": {},
   "source": [
    "### 7. Visualizing the Correlation Distribution\n",
    "This section visualizes the correlation between the proxy scores and the test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999ded2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Proxy Scores')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x=proxy_scores, y=accs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Proxy Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ad230",
   "metadata": {},
   "source": [
    "### 8. Identifying Top Architectures based on Proxies\n",
    "This section focuses on identifying the top-performing architectures based on the computed proxy scores.\n",
    "\n",
    "**Ranking by Proxy Scores*: We will sort the ViT architectures in descending order based on their proxy scores. This ranking prioritizes architectures with higher predicted performance.\n",
    "\n",
    "**Top Architectures**: We can then identify the top-ranked architectures (e.g., top 1 or top 5) as potential candidates for further exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d3b7b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision_TransformerSubnet(\n",
      "  (patch_embed_super): PatchembedSuper(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (drop_path): Identity()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=672, bias=True)\n",
      "      (fc2): LinearSuper(in_features=672, out_features=192, bias=True)\n",
      "    )\n",
      "    (1): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=672, bias=True)\n",
      "      (fc2): LinearSuper(in_features=672, out_features=192, bias=True)\n",
      "    )\n",
      "    (2): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=672, bias=True)\n",
      "      (fc2): LinearSuper(in_features=672, out_features=192, bias=True)\n",
      "    )\n",
      "    (3): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=768, bias=True)\n",
      "      (fc2): LinearSuper(in_features=768, out_features=192, bias=True)\n",
      "    )\n",
      "    (4): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=768, bias=True)\n",
      "      (fc2): LinearSuper(in_features=768, out_features=192, bias=True)\n",
      "    )\n",
      "    (5): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=768, bias=True)\n",
      "      (fc2): LinearSuper(in_features=768, out_features=192, bias=True)\n",
      "    )\n",
      "    (6): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=672, bias=True)\n",
      "      (fc2): LinearSuper(in_features=672, out_features=192, bias=True)\n",
      "    )\n",
      "    (7): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=768, bias=True)\n",
      "      (fc2): LinearSuper(in_features=768, out_features=192, bias=True)\n",
      "    )\n",
      "    (8): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=768, bias=True)\n",
      "      (fc2): LinearSuper(in_features=768, out_features=192, bias=True)\n",
      "    )\n",
      "    (9): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=672, bias=True)\n",
      "      (fc2): LinearSuper(in_features=672, out_features=192, bias=True)\n",
      "    )\n",
      "    (10): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=672, bias=True)\n",
      "      (fc2): LinearSuper(in_features=672, out_features=192, bias=True)\n",
      "    )\n",
      "    (11): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=672, bias=True)\n",
      "      (fc2): LinearSuper(in_features=672, out_features=192, bias=True)\n",
      "    )\n",
      "    (12): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=768, bias=True)\n",
      "      (fc2): LinearSuper(in_features=768, out_features=192, bias=True)\n",
      "    )\n",
      "    (13): TransformerEncoderLayer(\n",
      "      (drop_path): DropPath()\n",
      "      (attn): AttentionSuper(\n",
      "        (qkv): qkv_super(in_features=192, out_features=576, bias=True)\n",
      "        (rel_pos_embed_k): RelativePosition2D_super()\n",
      "        (rel_pos_embed_v): RelativePosition2D_super()\n",
      "        (proj): LinearSuper(in_features=192, out_features=192, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn_layer_norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): LinearSuper(in_features=192, out_features=768, bias=True)\n",
      "      (fc2): LinearSuper(in_features=768, out_features=192, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNormSuper((192,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): LinearSuper(in_features=192, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model.get_subnet_Vision_Transformer_Arch import get_subnet_arch\n",
    "best_index_architectures = np.argmax(proxy_scores) #get the index of top-1 architecture based on the proxy score\n",
    "\n",
    "# load the top-1 architecture and check the layer-wise details\n",
    "net_arch = arch_candidate_set[str(best_index_architectures)]['net_setting']\n",
    "model_best_by_proxys = get_subnet_arch(net_arch)\n",
    "\n",
    "\n",
    "print(model_best_by_proxys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025bde9",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[1] Chen, Minghao, Houwen Peng, Jianlong Fu, and Haibin Ling. \"Autoformer: Searching transformers for visual recognition.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12270-12280. 2021.\n",
    "\n",
    "[2] Li, Guihong, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, and Radu Marculescu. \"Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities.\" arXiv preprint arXiv:2307.01998 (2023).\n",
    "\n",
    "[3] Tanaka, Hidenori, Daniel Kunin, Daniel L. Yamins, and Surya Ganguli. \"Pruning neural networks without any data by iteratively conserving synaptic flow.\" Advances in neural information processing systems 33 (2020): 6377-6389."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoformerassign)",
   "language": "python",
   "name": "autoformerassign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
